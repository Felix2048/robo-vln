{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-11 01:00:47,596 Initializing dataset VLN-CE-v1\n",
      "2020-08-11 01:00:48,422 [construct_envs] Using GPU ID 0\n",
      "2020-08-11 01:00:48,423 [construct_envs] Using GPU ID 0\n",
      "2020-08-11 01:00:48,424 [construct_envs] Using GPU ID 0\n",
      "2020-08-11 01:00:48,424 [construct_envs] Using GPU ID 0\n",
      "2020-08-11 01:00:48,425 [construct_envs] Using GPU ID 0\n",
      "2020-08-11 01:00:48,426 [construct_envs] Using GPU ID 0\n"
     ]
    }
   ],
   "source": [
    "from vlnce_baselines.common.env_utils import construct_envs\n",
    "from habitat import Config\n",
    "from vlnce_baselines.config.default import get_config\n",
    "from habitat_baselines.common.environments import get_env_class\n",
    "\n",
    "#Import config and perform some manipulation\n",
    "exp_config ='vlnce_baselines/config/paper_configs/transformer_semantics.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "split = config.TASK_CONFIG.DATASET.SPLIT\n",
    "config.defrost()\n",
    "config.TASK_CONFIG.TASK.NDTW.SPLIT = split\n",
    "config.TASK_CONFIG.TASK.SDTW.SPLIT = split\n",
    "\n",
    "# if doing teacher forcing, don't switch the scene until it is complete\n",
    "if config.DAGGER.P == 1.0:\n",
    "    config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS = (\n",
    "        -1\n",
    "    )\n",
    "config.freeze()\n",
    "envs = construct_envs(config, get_env_class(config.ENV_NAME))\n",
    "obs=envs.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset and get observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Observations from env reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vlnce_baselines.common.utils import transform_obs\n",
    "from habitat_baselines.common.utils import batch_obs\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(envs.num_envs)\n",
    "print(envs.current_episodes()[0].episode_id)\n",
    "observations = envs.reset()\n",
    "# observations[0]['instruction_text'] = 'Go to kitchen'\n",
    "\n",
    "for k,v in observations[0].items():\n",
    "    print(k)\n",
    "# print(observations[0])\n",
    "\n",
    "observations = transform_obs(\n",
    "    observations, config.TASK_CONFIG.TASK.INSTRUCTION_SENSOR_UUID, is_bert=True\n",
    ")\n",
    "\n",
    "# print(observations)\n",
    "batch = batch_obs(observations, device=device)\n",
    "\n",
    "# print(batch['instruction_batch'])\n",
    "\n",
    "\n",
    "# print (batch[\"instruction_batch\"].shape)\n",
    "# lengths = (batch['instruction_batch'] != 0.0).long().sum(dim=1)\n",
    "# print(lengths)\n",
    "\n",
    "# print(batch['instruction'])\n",
    "instruction = batch[\"instruction\"].long()\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "embedding_layer = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "embedding_layer.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedded = embedding_layer(instruction)\n",
    "    embedded = embedded[0]\n",
    "     \n",
    "print(embedded.shape)\n",
    "\n",
    "lengths = (batch['instruction'] != 0.0).long().sum(dim=1)\n",
    "\n",
    "# def get_trasnformer_mask(instr_embedding, instr_len, device):\n",
    "#     mask = torch.ones((instr_embedding.shape[0], instr_embedding.shape[1]), dtype=torch.bool).to(device)\n",
    "#     attention_mask = torch.ones((instr_embedding.shape[0], instr_embedding.shape[1], instr_embedding.shape[1]), dtype=torch.bool).to(device)\n",
    "#     for i, _len in enumerate(instr_len):\n",
    "#         mask[i, :_len] = 0\n",
    "#         attention_mask[i, :_len, :_len] = 0\n",
    "#     pe_mask = mask.unsqueeze(dim=-1)\n",
    "#     value = (instr_embedding, pe_mask)\n",
    "#     return value, attention_mask.unsqueeze(1), mask.unsqueeze(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "# value, attn_mask, mask = get_trasnformer_mask(embedded, lengths, device)\n",
    "\n",
    "# print(\"attn_mask: \",attn_mask.shape)\n",
    "# print(\"mask: \",mask.shape)\n",
    "\n",
    "# print(value[1].shape)\n",
    "\n",
    "def get_instruction_mask(instr_embedding, instr_len, device):\n",
    "    mask = torch.ones((instr_embedding.shape[0], instr_embedding.shape[1]), dtype=torch.bool).to(device)\n",
    "    for i, _len in enumerate(instr_len):\n",
    "        mask[i, :_len] = 0\n",
    "    return mask.unsqueeze(dim=1).unsqueeze(dim=1)\n",
    "enc_mask = get_instruction_mask(embedded, lengths, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get observations from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['progress'].shape)\n",
    "\n",
    "print(batch['instruction'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import config and perform some manipulation\n",
    "from vlnce_baselines.config.default import get_config\n",
    "exp_config ='vlnce_baselines/config/paper_configs/transformer_semantics.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "split = config.TASK_CONFIG.DATASET.SPLIT\n",
    "config.defrost()\n",
    "config.TASK_CONFIG.TASK.NDTW.SPLIT = split\n",
    "config.TASK_CONFIG.TASK.SDTW.SPLIT = split\n",
    "\n",
    "# if doing teacher forcing, don't switch the scene until it is complete\n",
    "if config.DAGGER.P == 1.0:\n",
    "    config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS = (\n",
    "        -1\n",
    "    )\n",
    "config.freeze()\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import lmdb\n",
    "import msgpack_numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "class ObservationsDict(dict):\n",
    "    def pin_memory(self):\n",
    "        for k, v in self.items():\n",
    "            self[k] = v.pin_memory()\n",
    "\n",
    "        return self\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Each sample in batch: (\n",
    "            obs,\n",
    "            prev_actions,\n",
    "            oracle_actions,\n",
    "            inflec_weight,\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    def _pad_helper(t, max_len, fill_val=0):\n",
    "        pad_amount = max_len - t.size(0)\n",
    "        if pad_amount == 0:\n",
    "            return t\n",
    "        \n",
    "        pad = torch.full_like(t[0:1], fill_val).expand(pad_amount, *t.size()[1:])\n",
    "        return torch.cat([t, pad], dim=0)\n",
    "\n",
    "    transposed = list(zip(*batch))\n",
    "\n",
    "    observations_batch = list(transposed[0])\n",
    "    prev_actions_batch = list(transposed[1])\n",
    "    corrected_actions_batch = list(transposed[2])\n",
    "    N = len(corrected_actions_batch)\n",
    "    weights_batch = list(transposed[3])\n",
    "    B = len(prev_actions_batch)\n",
    "    new_observations_batch = defaultdict(list)\n",
    "    for sensor in observations_batch[0]:\n",
    "        if sensor == 'instruction':\n",
    "            for bid in range(N):\n",
    "                new_observations_batch[sensor].append(observations_batch[bid][sensor])\n",
    "        else: \n",
    "            for bid in range(B):\n",
    "                new_observations_batch[sensor].append(observations_batch[bid][sensor])\n",
    "\n",
    "    observations_batch = new_observations_batch\n",
    "\n",
    "    max_traj_len = max(ele.size(0) for ele in prev_actions_batch)\n",
    "#     obs_lengths=[]\n",
    "    for bid in range(B):\n",
    "        for sensor in observations_batch:\n",
    "            if sensor == 'instruction':\n",
    "                continue\n",
    "            observations_batch[sensor][bid] = _pad_helper(\n",
    "                observations_batch[sensor][bid], max_traj_len, fill_val=0.0\n",
    "            )\n",
    "#         obs_lengths.append(prev_actions_batch[bid].shape[0])\n",
    "        prev_actions_batch[bid] = _pad_helper(prev_actions_batch[bid], max_traj_len)\n",
    "        corrected_actions_batch[bid] = _pad_helper(\n",
    "            corrected_actions_batch[bid], max_traj_len, fill_val=-1.0\n",
    "        )\n",
    "        weights_batch[bid] = _pad_helper(weights_batch[bid], max_traj_len)\n",
    "\n",
    "    for sensor in observations_batch:\n",
    "        observations_batch[sensor] = torch.stack(observations_batch[sensor], dim=1)\n",
    "        observations_batch[sensor] = observations_batch[sensor].transpose(1,0)\n",
    "        observations_batch[sensor] = observations_batch[sensor].contiguous().view(\n",
    "            -1, *observations_batch[sensor].size()[2:]\n",
    "        )\n",
    "\n",
    "    prev_actions_batch = torch.stack(prev_actions_batch, dim=1)\n",
    "    corrected_actions_batch = torch.stack(corrected_actions_batch, dim=1)\n",
    "    weights_batch = torch.stack(weights_batch, dim=1)\n",
    "    not_done_masks = torch.ones_like(corrected_actions_batch, dtype=torch.float)\n",
    "    not_done_masks[0] = 0\n",
    "    \n",
    "    prev_actions_batch = prev_actions_batch.transpose(1,0)\n",
    "    not_done_masks = not_done_masks.transpose(1,0)\n",
    "    corrected_actions_batch = corrected_actions_batch.transpose(1,0)\n",
    "    weights_batch = weights_batch.transpose(1,0)\n",
    "    \n",
    "    observations_batch = ObservationsDict(observations_batch)\n",
    "\n",
    "    return (\n",
    "        observations_batch,\n",
    "        prev_actions_batch.contiguous().view(-1, 1),\n",
    "        not_done_masks.contiguous().view(-1, 1),\n",
    "        corrected_actions_batch,\n",
    "        weights_batch,\n",
    "    )\n",
    "\n",
    "def _block_shuffle(lst, block_size):\n",
    "    blocks = [lst[i : i + block_size] for i in range(0, len(lst), block_size)]\n",
    "    random.shuffle(blocks)\n",
    "\n",
    "    return [ele for block in blocks for ele in block]\n",
    "\n",
    "class IWTrajectoryDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lmdb_features_dir,\n",
    "        use_iw,\n",
    "        inflection_weight_coef=1.0,\n",
    "        lmdb_map_size=1e9,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lmdb_features_dir = lmdb_features_dir\n",
    "        self.lmdb_map_size = lmdb_map_size\n",
    "        self.preload_size = batch_size * 100\n",
    "        self._preload = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if use_iw:\n",
    "            self.inflec_weights = torch.tensor([1.0, inflection_weight_coef])\n",
    "        else:\n",
    "            self.inflec_weights = torch.tensor([1.0, 1.0])\n",
    "\n",
    "        with lmdb.open(\n",
    "            self.lmdb_features_dir,\n",
    "            map_size=int(self.lmdb_map_size),\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "        ) as lmdb_env:\n",
    "            self.length = lmdb_env.stat()[\"entries\"]\n",
    "\n",
    "    def _load_next(self):\n",
    "        if len(self._preload) == 0:\n",
    "            if len(self.load_ordering) == 0:\n",
    "                raise StopIteration\n",
    "\n",
    "            new_preload = []\n",
    "            lengths = []\n",
    "            with lmdb.open(\n",
    "                self.lmdb_features_dir,\n",
    "                map_size=int(self.lmdb_map_size),\n",
    "                readonly=True,\n",
    "                lock=False,\n",
    "            ) as lmdb_env, lmdb_env.begin(buffers=True) as txn:\n",
    "                for _ in range(self.preload_size):\n",
    "                    if len(self.load_ordering) == 0:\n",
    "                        break\n",
    "\n",
    "                    new_preload.append(\n",
    "                        msgpack_numpy.unpackb(\n",
    "                            txn.get(str(self.load_ordering.pop()).encode()), raw=False\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    lengths.append(len(new_preload[-1][0]))\n",
    "\n",
    "            sort_priority = list(range(len(lengths)))\n",
    "            random.shuffle(sort_priority)\n",
    "\n",
    "            sorted_ordering = list(range(len(lengths)))\n",
    "            sorted_ordering.sort(key=lambda k: (lengths[k], sort_priority[k]))\n",
    "\n",
    "            for idx in _block_shuffle(sorted_ordering, self.batch_size):\n",
    "                self._preload.append(new_preload[idx])\n",
    "\n",
    "        return self._preload.pop()\n",
    "\n",
    "    def __next__(self):\n",
    "        obs, prev_actions, oracle_actions= self._load_next()\n",
    "        \n",
    "        instruction_batch = obs['instruction'][0]\n",
    "        instruction_batch = np.expand_dims(instruction_batch, axis=0)\n",
    "        obs['instruction'] = instruction_batch\n",
    "        for k, v in obs.items():\n",
    "            obs[k] = torch.from_numpy(v)\n",
    "\n",
    "        prev_actions = torch.from_numpy(prev_actions)\n",
    "        \n",
    "        \n",
    "        oracle_actions = torch.from_numpy(oracle_actions)\n",
    "\n",
    "        inflections = torch.cat(\n",
    "            [\n",
    "                torch.tensor([1], dtype=torch.long),\n",
    "                (oracle_actions[1:] != oracle_actions[:-1]).long(),\n",
    "            ]\n",
    "        )\n",
    "        return (obs, prev_actions, oracle_actions, self.inflec_weights[inflections])\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            start = 0\n",
    "            end = self.length\n",
    "        else:\n",
    "            per_worker = int(np.ceil(self.length / worker_info.num_workers))\n",
    "\n",
    "            start = per_worker * worker_info.id\n",
    "            end = min(start + per_worker, self.length)\n",
    "\n",
    "        # Reverse so we can use .pop()\n",
    "        self.load_ordering = list(\n",
    "            reversed(_block_shuffle(list(range(start, end)), self.preload_size))\n",
    "        )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428091666/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/opt/conda/conda-bld/pytorch_1587428091666/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/opt/conda/conda-bld/pytorch_1587428091666/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction\n",
      "progress\n",
      "heading\n",
      "ego_sem_map\n",
      "rgb_features\n",
      "depth_features\n"
     ]
    }
   ],
   "source": [
    "# lmdb_features_dir = 'data/trajectories_dirs/seq2seq/trajectories.lmdb'\n",
    "lmdb_features_dir = '/data/zirshad/VLNCE-data/data/trajectory_dir/transformer_semantic/trajectories.lmdb'\n",
    "\n",
    "USE_IW= False\n",
    "\n",
    "dataset = IWTrajectoryDataset(\n",
    "    lmdb_features_dir,\n",
    "    USE_IW,\n",
    "    inflection_weight_coef=1.0,\n",
    "    lmdb_map_size=1e9,\n",
    "    batch_size=5,\n",
    ")\n",
    "diter = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=5,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,  # drop last batch if smaller\n",
    "    num_workers=3,\n",
    ")\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(dataset.length)\n",
    "\n",
    "# iterloader = iter(diter)\n",
    "observations_batch,prev_actions_batch,not_done_masks,corrected_actions_batch,weights_batch = next(iter(diter))\n",
    "# try:\n",
    "#     observations_batch,prev_actions_batch,not_done_masks,corrected_actions_batch,weights_batch = next(iterloader)\n",
    "# except StopIteration:\n",
    "#     batch_iter = iter(diter)\n",
    "#     observations_batch,prev_actions_batch,not_done_masks,corrected_actions_batch,weights_batch = next(batch_iter)\n",
    "    \n",
    "# print(observations_batch['depth_features'].shape)\n",
    "# print(prev_actions_batch.shape)\n",
    "observations_batch = {\n",
    "    k: v.to(device=device, non_blocking=True)\n",
    "    for k, v in observations_batch.items()\n",
    "}\n",
    "\n",
    "for k, v in observations_batch.items():\n",
    "    print(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([490, 40, 40])\n",
      "torch.Size([5, 98, 40, 40])\n",
      "torch.Size([5, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "print(observations_batch['ego_sem_map'].shape)\n",
    "\n",
    "T,N = corrected_actions_batch.size()\n",
    "\n",
    "esm= observations_batch['ego_sem_map']\n",
    "sem_map          = esm.view(T, N, *esm.size()[1:])\n",
    "\n",
    "print(sem_map.shape)\n",
    "\n",
    "print(sem_map[:,10].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([490, 1])\n",
      "torch.Size([5, 98])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "tensor([2, 2, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "pab= prev_actions_batch.view(T,N)\n",
    "\n",
    "\n",
    "print(not_done_masks.shape)\n",
    "ma = not_done_masks.view(T,N)\n",
    "\n",
    "print(ma.shape)\n",
    "# print(prev_actions_batch.view(T,N))\n",
    "\n",
    "prev_actions_single = pab[:,3]\n",
    "\n",
    "\n",
    "print(prev_actions_single.shape)\n",
    "\n",
    "print(ma[:,1].shape)\n",
    "print(prev_actions_single)\n",
    "\n",
    "\n",
    "# print(corrected_actions_batch)\n",
    "\n",
    "corrected_actions = corrected_actions_batch.contiguous().view(T*N)\n",
    "\n",
    "# print(corrected_actions)\n",
    "\n",
    "prev_action_embedding = nn.Embedding(num_actions+1, 32, padding_idx= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 3, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (81) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e04fbd89abbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(masks_single)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpad_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_actions_single\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmasks_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (81) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "prev_actions_single = pab[:,1]\n",
    "\n",
    "print(prev_actions_single)\n",
    "masks_single = ma[:,:80+1]\n",
    "\n",
    "# print(prev_actions_single)\n",
    "# print(masks_single)\n",
    "\n",
    "pad_mask = (((prev_actions_single+1)*masks_single)!= 1).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "print(pad_mask)\n",
    "\n",
    "seq_len = prev_actions_single.shape[1]\n",
    "\n",
    "# prev_action_pad = ((prev_actions_batch+1)*not_done_masks).view(T,N)\n",
    "\n",
    "# pad= prev_action_pad[:,0:80+1]\n",
    "\n",
    "# prev_action_pad_mask = (pad != 1).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "print(prev_action_pad_mask)\n",
    "\n",
    "# print(prev_action_pad_mask)\n",
    "mask_self_attention = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).bool()\n",
    "# print(torch.triu(torch.ones((seq_len, seq_len)), difagonal=1))\n",
    "mask_self_attention = mask_self_attention  # (1, 1, seq_len, seq_len)\n",
    "mask_self_attention = mask_self_attention.gt(0)  # (b_s, 1, seq_len, seq_len)\n",
    "\n",
    "mask = prev_action_pad_mask & ~mask_self_attention\n",
    "mask = ~mask\n",
    "print(\"----------------------\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations_batch['progress'].shape)\n",
    "\n",
    "print(observations_batch['instruction'].shape)\n",
    "\n",
    "print(prev_actions_batch.shape)\n",
    "\n",
    "print(corrected_actions_batch.shape)\n",
    "\n",
    "T,N = corrected_actions_batch.size()\n",
    "# df = observations_batch['depth_features'].view(T,N,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=observations_batch\n",
    "instruction = batch[\"instruction\"].long()\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "embedding_layer = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "embedding_layer.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedded = embedding_layer(instruction)\n",
    "    embedded = embedded[0]\n",
    "     \n",
    "print(embedded.shape)\n",
    "\n",
    "lengths = (batch['instruction'] != 0.0).long().sum(dim=1)\n",
    "\n",
    "def get_trasnformer_mask(instr_embedding, instr_len, device):\n",
    "    mask = torch.ones((instr_embedding.shape[0], instr_embedding.shape[1]), dtype=torch.bool).to(device)\n",
    "    attention_mask = torch.ones((instr_embedding.shape[0], instr_embedding.shape[1], instr_embedding.shape[1]), dtype=torch.bool).to(device)\n",
    "    for i, _len in enumerate(instr_len):\n",
    "        mask[i, :_len] = 0\n",
    "        attention_mask[i, :_len, :_len] = 0\n",
    "    pe_mask = mask.unsqueeze(dim=-1)\n",
    "    value = (instr_embedding, pe_mask)\n",
    "    return value, attention_mask.unsqueeze(1), mask.unsqueeze(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "value, attn_mask, mask = get_trasnformer_mask(embedded, lengths, device)\n",
    "\n",
    "print(\"attn_mask: \",attn_mask.shape)\n",
    "print(\"mask: \",mask.shape)\n",
    "\n",
    "print(value[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# print(prev_actions_batch)\n",
    "\n",
    "# print(prev_actions_batch.float() + 1)\n",
    "\n",
    "# print(not_done_masks)\n",
    "\n",
    "# print(prev_actions_batch)\n",
    "\n",
    "\n",
    "num_embeddings = envs.action_spaces[0].n\n",
    "\n",
    "print(num_embeddings)\n",
    "embedding = nn.Embedding(num_actions+1, 32, padding_idx= 1)\n",
    "\n",
    "prev_actions_embedding = embedding(((prev_actions_batch.float()+1) * not_done_masks).long())\n",
    "\n",
    "print(prev_actions_embedding.shape)\n",
    "T,N = corrected_actions_batch.size()\n",
    "print(prev_actions_embedding[74])\n",
    "print(T,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Language encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlnce_baselines.models.transformer.transformer import TransformerLanguageEncoder\n",
    "d_model = 512\n",
    "dropout =0.1\n",
    "h=8\n",
    "d_att   = int(d_model / h)\n",
    "\n",
    "model_config = config.MODEL\n",
    "encoder = TransformerLanguageEncoder(model_config.TRANSFORMER_INSTRUCTION_ENCODER).to(device=device)\n",
    "\n",
    "w_t = encoder(value, attention_mask=attn_mask, attention_weights=None, device=device)\n",
    "\n",
    "print(w_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config.MODEL\n",
    "ins_fc =  torch.nn.Linear(model_config.TRANSFORMER_INSTRUCTION_ENCODER.d_in, model_config.TRANSFORMER_INSTRUCTION_ENCODER.d_model).to(device)\n",
    "w_t = ins_fc(embedded)\n",
    "\n",
    "print(w_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from vlnce_baselines.common.utils import sinusoid_encoding_table\n",
    "\n",
    "d_in = 768\n",
    "dropout =0.1\n",
    "fc = nn.Linear(d_in, d_model, bias=True).to(device)\n",
    "data, mask = value\n",
    "dropout = nn.Dropout(p=dropout)\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "out = F.relu(fc(data))\n",
    "out = dropout(out)\n",
    "out = layer_norm(out)\n",
    "pe = sinusoid_encoding_table(out.shape[1], out.shape[2])\n",
    "pe = pe.expand(out.shape[0], pe.shape[0], pe.shape[1]).to(device)\n",
    "\n",
    "print(pe.shape)\n",
    "out = out + pe.masked_fill(mask, 0)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vlnce_baselines.common.utils import sinusoid_encoding_table\n",
    "\n",
    "pe = sinusoid_encoding_table(out.shape[1], out.shape[2])\n",
    "\n",
    "print(pe.shape)\n",
    "\n",
    "pos = torch.arange(out.shape[1], dtype=torch.float32)\n",
    "\n",
    "dim = torch.arange(out.shape[2] // 2, dtype=torch.float32).view(1, -1)\n",
    "\n",
    "\n",
    "\n",
    "pos = pos.view(-1, 1)\n",
    "print(pos.shape)\n",
    "print(dim.shape)\n",
    "\n",
    "sin = torch.sin(pos / 10000 ** (2 * dim / out.shape[2]))\n",
    "\n",
    "print(sin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model1 = torchvision.models.resnet18(pretrained=True)\n",
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETR Backbone and Positional Encodings for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['rgb'].shape)\n",
    "from typing import Optional, List\n",
    "from torch import Tensor\n",
    "import torch.distributed as dist\n",
    "\n",
    "class NestedTensor(object):\n",
    "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "        self.tensors = tensors\n",
    "        self.mask = mask\n",
    "\n",
    "    def to(self, device):\n",
    "        # type: (Device) -> NestedTensor # noqa\n",
    "        cast_tensor = self.tensors.to(device)\n",
    "        mask = self.mask\n",
    "        if mask is not None:\n",
    "            assert mask is not None\n",
    "            cast_mask = mask.to(device)\n",
    "        else:\n",
    "            cast_mask = None\n",
    "        return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "    def decompose(self):\n",
    "        return self.tensors, self.mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.tensors)\n",
    "    \n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "\n",
    "def _max_by_axis(the_list):\n",
    "    # type: (List[List[int]]) -> List[int]\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "    return maxes\n",
    "\n",
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    # TODO make this more general\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        # TODO make it support different-sized images\n",
    "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
    "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return NestedTensor(tensor, mask)\n",
    "\n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "class BackboneBase(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n",
    "        super().__init__()\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
    "                parameter.requires_grad_(False)\n",
    "        if return_interm_layers:\n",
    "            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
    "        else:\n",
    "            return_layers = {'layer4': \"0\"}\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.body(x)\n",
    "#         out: Dict[str, NestedTensor] = {}\n",
    "#         for name, x in xs.items():\n",
    "#             m = tensor_list.mask\n",
    "#             assert m is not None\n",
    "#             mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
    "#             out[name] = NestedTensor(x, mask)\n",
    "        return xs\n",
    "\n",
    "class Backbone(BackboneBase):\n",
    "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_backbone: bool,\n",
    "                 return_interm_layers: bool,\n",
    "                 dilation: bool):\n",
    "        backbone = getattr(torchvision.models, name)(\n",
    "            replace_stride_with_dilation=[False, False, dilation],\n",
    "            pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d)\n",
    "        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
    "        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)\n",
    "        \n",
    "\n",
    "lr_backbone = 1e-5\n",
    "\n",
    "backbone ='resnet50'\n",
    "train_backbone = lr_backbone > 0\n",
    "\n",
    "return_interm_layers = False\n",
    "dilation = False\n",
    "\n",
    "backbone = Backbone(backbone, train_backbone, return_interm_layers, dilation).to(device)\n",
    "\n",
    "# if isinstance(samples, (list, torch.Tensor)):\n",
    "#     samples = nested_tensor_from_tensor_list(samples)\n",
    "\n",
    "        \n",
    "samples = batch['rgb'].permute(0,3,1,2)\n",
    "\n",
    "features = backbone(samples)\n",
    "\n",
    "print(features['0'].shape)\n",
    "\n",
    "\n",
    "num_channels = 2048\n",
    "\n",
    "hidden_dim = 512//2\n",
    "input_proj = nn.Conv2d(num_channels, hidden_dim, kernel_size=1).to(device)\n",
    "\n",
    "out = input_proj(features['0'])\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "src = out.flatten(2).permute(2, 0, 1)\n",
    "print(src.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim =256\n",
    "N_steps = hidden_dim // 2\n",
    "\n",
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"\n",
    "    Absolute pos embedding, learned.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=256):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight)\n",
    "        nn.init.uniform_(self.col_embed.weight)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        x = tensor\n",
    "        h, w = x.shape[-2:]\n",
    "        i = torch.arange(w, device=x.device)\n",
    "        \n",
    "        print(\"i\",i.shape)\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        print(\"j\", j.shape)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        \n",
    "        print(x_emb.shape)\n",
    "        print(y_emb.shape)\n",
    "        pos = torch.cat([\n",
    "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
    "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
    "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
    "        return pos\n",
    "\n",
    "position_embedding = PositionEmbeddingLearned(N_steps).to(device)\n",
    "\n",
    "pos = position_embedding(out)\n",
    "\n",
    "print(pos.shape)\n",
    "\n",
    "\n",
    "pos_embed = pos.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "print(pos_embed.shape)\n",
    "\n",
    "# o = pos_embed +src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "\n",
    "print(input.shape)\n",
    "embed= embedding(input)\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "observation_space=envs.observation_spaces[0]\n",
    "action_space=envs.action_spaces[0]\n",
    "\n",
    "model_config = config.MODEL\n",
    "\n",
    "class TorchVisionResNet50(nn.Module):\n",
    "    r\"\"\"\n",
    "    Takes in observations and produces an embedding of the rgb component.\n",
    "\n",
    "    Args:\n",
    "        observation_space: The observation_space of the agent\n",
    "        output_size: The size of the embedding vector\n",
    "        device: torch.device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, observation_space, output_size, device, spatial_output: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.resnet_layer_size = 2048\n",
    "        linear_layer_input_size = 0\n",
    "        if \"rgb\" in observation_space.spaces:\n",
    "            self._n_input_rgb = observation_space.spaces[\"rgb\"].shape[2]\n",
    "            obs_size_0 = observation_space.spaces[\"rgb\"].shape[0]\n",
    "            obs_size_1 = observation_space.spaces[\"rgb\"].shape[1]\n",
    "            if obs_size_0 != 224 or obs_size_1 != 224:\n",
    "                logger.warn(\n",
    "                    f\"WARNING: TorchVisionResNet50: observation size {obs_size} is not conformant to expected ResNet input size [3x224x224]\"\n",
    "                )\n",
    "            linear_layer_input_size += self.resnet_layer_size\n",
    "        else:\n",
    "            self._n_input_rgb = 0\n",
    "\n",
    "        if self.is_blind:\n",
    "            self.cnn = nn.Sequential()\n",
    "            return\n",
    "\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "\n",
    "        # disable gradients for resnet, params frozen\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.cnn.eval()\n",
    "\n",
    "        self.spatial_output = spatial_output\n",
    "\n",
    "        if not self.spatial_output:\n",
    "            self.output_shape = (output_size,)\n",
    "            self.fc = nn.Linear(linear_layer_input_size, output_size).cuda(2)\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "\n",
    "            class SpatialAvgPool(nn.Module):\n",
    "                def forward(self, x):\n",
    "                    x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "\n",
    "                    return x\n",
    "\n",
    "            self.cnn.avgpool = SpatialAvgPool()\n",
    "            self.cnn.fc = nn.Sequential()\n",
    "\n",
    "            self.spatial_embeddings = nn.Embedding(4 * 4, 64)\n",
    "\n",
    "            self.output_shape = (\n",
    "                self.resnet_layer_size + self.spatial_embeddings.embedding_dim,\n",
    "                4,\n",
    "                4,\n",
    "            )\n",
    "\n",
    "        self.layer_extract = self.cnn._modules.get(\"layer4\")\n",
    "\n",
    "    @property\n",
    "    def is_blind(self):\n",
    "        return self._n_input_rgb == 0\n",
    "\n",
    "    def forward(self, observations):\n",
    "        r\"\"\"Sends RGB observation through the TorchVision ResNet50 pre-trained\n",
    "        on ImageNet. Sends through fully connected layer, activates, and\n",
    "        returns final embedding.\n",
    "        \"\"\"\n",
    "\n",
    "        def resnet_forward(observation):\n",
    "            resnet_output = torch.zeros(1, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            def hook(m, i, o):\n",
    "                resnet_output.set_(o)\n",
    "\n",
    "            # output: [BATCH x RESNET_DIM]\n",
    "            h = self.layer_extract.register_forward_hook(hook)\n",
    "            self.cnn(observation)\n",
    "            h.remove()\n",
    "            return resnet_output\n",
    "\n",
    "        if \"rgb_features\" in observations:\n",
    "            resnet_output = observations[\"rgb_features\"]\n",
    "        else:\n",
    "            # permute tensor to dimension [BATCH x CHANNEL x HEIGHT x WIDTH]\n",
    "            rgb_observations = observations[\"rgb\"].permute(0, 3, 1, 2)\n",
    "            rgb_observations = rgb_observations / 255.0  # normalize RGB\n",
    "            resnet_output = resnet_forward(rgb_observations.contiguous())\n",
    "\n",
    "        if self.spatial_output:\n",
    "            b, c, h, w = resnet_output.size()\n",
    "\n",
    "            spatial_features = (\n",
    "                self.spatial_embeddings(\n",
    "                    torch.arange(\n",
    "                        0,\n",
    "                        self.spatial_embeddings.num_embeddings,\n",
    "                        device=resnet_output.device,\n",
    "                        dtype=torch.long,\n",
    "                    )\n",
    "                )\n",
    "                .view(1, -1, h, w)\n",
    "                .expand(b, self.spatial_embeddings.embedding_dim, h, w)\n",
    "            )\n",
    "\n",
    "            return torch.cat([resnet_output, spatial_features], dim=1)\n",
    "        else:\n",
    "            return resnet_output\n",
    "        \n",
    "        \n",
    "rgb_encoder = TorchVisionResNet50(\n",
    "                observation_space, model_config.RGB_ENCODER.output_size, device, spatial_output=False,\n",
    "            )\n",
    "\n",
    "rgb_encoder = rgb_encoder.to(device)\n",
    "rgb_embedding = rgb_encoder(batch)\n",
    "\n",
    "num_channels = 2048\n",
    "\n",
    "hidden_dim = 512\n",
    "\n",
    "input_proj = nn.Conv2d(num_channels, hidden_dim, kernel_size=1).to(device)\n",
    "\n",
    "print(rgb_embedding.shape)\n",
    "\n",
    "out = input_proj(rgb_embedding)\n",
    "\n",
    "out = out.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim =512\n",
    "N_steps = hidden_dim // 2\n",
    "\n",
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"\n",
    "    Absolute pos embedding, learned.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=256):\n",
    "        super().__init__()\n",
    "        self.row_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.col_embed = nn.Embedding(50, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.row_embed.weight)\n",
    "        nn.init.uniform_(self.col_embed.weight)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        x = tensor\n",
    "        h, w = x.shape[-2:]\n",
    "        i = torch.arange(w, device=x.device)\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        print(\"i: \",i.shape)\n",
    "        print(\"j: \",j.shape)\n",
    "        x_emb = self.col_embed(i)\n",
    "        y_emb = self.row_embed(j)\n",
    "        \n",
    "        print(\"x_embed\",x_emb.shape)\n",
    "        print(\"y_embed\",x_emb.shape)\n",
    "        \n",
    "        print(\"x_repeat: \", x_emb.unsqueeze(0).repeat(h, 1, 1).shape)\n",
    "        print(\"y_repeat: \", y_emb.unsqueeze(1).repeat(1, w, 1).shape)\n",
    "        pos = torch.cat([\n",
    "            x_emb.unsqueeze(0).repeat(h, 1, 1),\n",
    "            y_emb.unsqueeze(1).repeat(1, w, 1),\n",
    "        ], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n",
    "        return pos\n",
    "\n",
    "position_embedding = PositionEmbeddingLearned(N_steps).to(device)\n",
    "\n",
    "pos = position_embedding(rgb_embedding)\n",
    "\n",
    "print(pos.shape)\n",
    "\n",
    "\n",
    "pos_embed = pos.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "print(pos_embed.shape)\n",
    "\n",
    "# combined = pos_embed +out\n",
    "\n",
    "# print(combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from habitat_baselines.rl.ddppo.policy.resnet_policy import ResNetEncoder\n",
    "from gym import spaces\n",
    "from habitat_baselines.rl.ddppo.policy import resnet\n",
    "from habitat_baselines.common.utils import Flatten\n",
    "import numpy as np\n",
    "class VlnResnetDepthEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        output_size=128,\n",
    "        checkpoint=\"NONE\",\n",
    "        backbone=\"resnet50\",\n",
    "        resnet_baseplanes=32,\n",
    "        normalize_visual_inputs=False,\n",
    "        trainable=False,\n",
    "        spatial_output: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.visual_encoder = ResNetEncoder(\n",
    "            spaces.Dict({\"depth\": observation_space.spaces[\"depth\"]}),\n",
    "            baseplanes=resnet_baseplanes,\n",
    "            ngroups=resnet_baseplanes // 2,\n",
    "            make_backbone=getattr(resnet, backbone),\n",
    "            normalize_visual_inputs=normalize_visual_inputs,\n",
    "        )\n",
    "\n",
    "        for param in self.visual_encoder.parameters():\n",
    "            param.requires_grad_(trainable)\n",
    "\n",
    "        if checkpoint != \"NONE\":\n",
    "            ddppo_weights = torch.load(checkpoint)\n",
    "\n",
    "            weights_dict = {}\n",
    "            for k, v in ddppo_weights[\"state_dict\"].items():\n",
    "                split_layer_name = k.split(\".\")[2:]\n",
    "                if split_layer_name[0] != \"visual_encoder\":\n",
    "                    continue\n",
    "\n",
    "                layer_name = \".\".join(split_layer_name[1:])\n",
    "                weights_dict[layer_name] = v\n",
    "\n",
    "            del ddppo_weights\n",
    "            self.visual_encoder.load_state_dict(weights_dict, strict=True)\n",
    "\n",
    "        self.spatial_output = spatial_output\n",
    "\n",
    "        if not self.spatial_output:\n",
    "            self.output_shape = (output_size,)\n",
    "            self.visual_fc = nn.Sequential(\n",
    "                Flatten(),\n",
    "                nn.Linear(np.prod(self.visual_encoder.output_shape), output_size),\n",
    "                nn.ReLU(True),\n",
    "            ).cuda(2)\n",
    "        else:\n",
    "            self.spatial_embeddings = nn.Embedding(\n",
    "                self.visual_encoder.output_shape[1]\n",
    "                * self.visual_encoder.output_shape[2],\n",
    "                64,\n",
    "            )\n",
    "\n",
    "            self.output_shape = list(self.visual_encoder.output_shape)\n",
    "            self.output_shape[0] += self.spatial_embeddings.embedding_dim\n",
    "            self.output_shape = tuple(self.output_shape)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: [BATCH, HEIGHT, WIDTH, CHANNEL]\n",
    "        Returns:\n",
    "            [BATCH, OUTPUT_SIZE]\n",
    "        \"\"\"\n",
    "        if \"depth_features\" in observations:\n",
    "            x = observations[\"depth_features\"]\n",
    "        else:\n",
    "            x = self.visual_encoder(observations)\n",
    "\n",
    "        if self.spatial_output:\n",
    "            b, c, h, w = x.size()\n",
    "\n",
    "            spatial_features = (\n",
    "                self.spatial_embeddings(\n",
    "                    torch.arange(\n",
    "                        0,\n",
    "                        self.spatial_embeddings.num_embeddings,\n",
    "                        device=x.device,\n",
    "                        dtype=torch.long,\n",
    "                    )\n",
    "                )\n",
    "                .view(1, -1, h, w)\n",
    "                .expand(b, self.spatial_embeddings.embedding_dim, h, w)\n",
    "            )\n",
    "\n",
    "            return torch.cat([x, spatial_features], dim=1)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "depth_encoder = VlnResnetDepthEncoder(\n",
    "                observation_space,\n",
    "                output_size=model_config.DEPTH_ENCODER.output_size,\n",
    "                checkpoint=model_config.DEPTH_ENCODER.ddppo_checkpoint,\n",
    "                backbone=model_config.DEPTH_ENCODER.backbone,\n",
    "                spatial_output=False,\n",
    "            ).to(device)\n",
    "        \n",
    "depth_embedding = depth_encoder(batch)\n",
    "\n",
    "print(depth_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "model_1 = resnet50(pretrained=True)\n",
    "\n",
    "print(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Depth Resnet Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "observations_batch = batch\n",
    "model_config = config.MODEL\n",
    "\n",
    "from vlnce_baselines.models.encoders.resnet_encoders import (\n",
    "    TorchVisionResNet50,\n",
    "    VlnResnetDepthEncoder,\n",
    ")\n",
    "observation_space=envs.observation_spaces[0]\n",
    "action_space=envs.action_spaces[0]\n",
    "depth_encoder = VlnResnetDepthEncoder(\n",
    "                observation_space,\n",
    "                output_size=model_config.DEPTH_ENCODER.output_size,\n",
    "                checkpoint=model_config.DEPTH_ENCODER.ddppo_checkpoint,\n",
    "                backbone=model_config.DEPTH_ENCODER.backbone,\n",
    "                resnet_output=True,\n",
    "            )\n",
    "rgb_encoder = TorchVisionResNet50(\n",
    "                observation_space, model_config.RGB_ENCODER.output_size,model_config.RGB_ENCODER.resnet_output_size, device, resnet_output=True,\n",
    "            )\n",
    "\n",
    "rgb_encoder = rgb_encoder.to(device)\n",
    "depth_encoder = depth_encoder.to(device)\n",
    "\n",
    "depth_embedding = depth_encoder(observations_batch)\n",
    "print(depth_embedding.shape)\n",
    "rgb_embedding = rgb_encoder(observations_batch)\n",
    "print(rgb_embedding.shape)\n",
    "# x = torch.cat([depth_embedding, rgb_embedding], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations_batch['rgb_features'].shape)\n",
    "print(observations_batch['depth_features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "rgb_d = torch.cat((rgb_embedding, depth_embedding), dim=1)\n",
    "\n",
    "print(rgb_d.shape)\n",
    "\n",
    "fc = torch.nn.Linear(rgb_embedding.shape[1]+depth_embedding.shape[1], 512).to(device)\n",
    "\n",
    "rgb_d = F.relu(fc(rgb_d.permute(0,2,3,1)))\n",
    "\n",
    "rgb_d = rgb_d.permute(0,3,1,2)\n",
    "\n",
    "\n",
    "print(rgb_d.shape)\n",
    "\n",
    "# rgb_embedding = rgb_embedding.view(batch_size, max_len, -1, rgb_out_dim, rgb_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(rgb_embedding.shape)\n",
    "\n",
    "print(depth_embedding.shape)\n",
    "\n",
    "pooler = torch.nn.AdaptiveAvgPool2d(4).to(device)\n",
    "\n",
    "rgb_up = pooler(rgb_embedding)\n",
    "\n",
    "print(rgb_up.shape)\n",
    "\n",
    "# rgb_embedding = rgb_embedding[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D POS Encoding for Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "print(rgb_embedding.shape)\n",
    "\n",
    "from vlnce_baselines.models.transformer.transformer import PositionEmbedding2DLearned\n",
    "\n",
    "N_steps = model_config.IMAGE_CROSS_MODAL_ENCODER.d_model // 2\n",
    "position_embedding_2d = PositionEmbedding2DLearned(N_steps).to(device)\n",
    "rgbd_pos_embed =  position_embedding_2d(rgb_d)\n",
    "\n",
    "print(rgbd_pos_embed.shape)\n",
    "\n",
    "rgbd_out = rgb_d.flatten(2).permute(0, 2, 1)\n",
    "pos_embed_out = rgbd_pos_embed.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "print(rgbd_out.shape)\n",
    "print(pos_embed_out.shape)\n",
    "# num_channels = 2048\n",
    "\n",
    "# hidden_dim = 512\n",
    "\n",
    "# input_proj = nn.Conv2d(num_channels, hidden_dim, kernel_size=1).to(device)\n",
    "\n",
    "# rgb = input_proj(rgb_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Encoder with Self and Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from vlnce_baselines.models.transformer.transformer import ImageCrossModalEncoder, ImageEncoder_with_PosEncodings, SemMapEncoder_with_PosEncodings\n",
    "\n",
    "image_encoder_trans = ImageEncoder_with_PosEncodings(model_config.IMAGE_CROSS_MODAL_ENCODER).to(device=device)\n",
    "\n",
    "i_t = image_encoder_trans(rgbd_out, w_t, None, enc_mask, pos_embed_out)\n",
    "\n",
    "print(i_t.shape)\n",
    "\n",
    "list =[]\n",
    "\n",
    "visual_pooler = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool1d((1)),\n",
    "    nn.Flatten()\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# i_t = visual_pooler(i_t.permute(0,2,1))\n",
    "\n",
    "# print(i_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(batch['ego_sem_map'].shape)\n",
    "print(w_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Map Self and Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space=envs.observation_spaces[0]\n",
    "action_space=envs.action_spaces[0]\n",
    "import torch.nn.functional as F\n",
    "from vlnce_baselines.models.transformer.transformer import SemMapEncoder_with_PosEncodings\n",
    "model_config.defrost()\n",
    "model_config.SEM_MAP_TRANSFORMER.map_size = int(observation_space.spaces[\"ego_sem_map\"].high.max() + 1)\n",
    "model_config.SEM_MAP_TRANSFORMER.max_position_embeddings=observation_space.spaces[\"ego_sem_map\"].shape[0] // 2\n",
    "model_config.freeze()\n",
    "\n",
    "sem_map_encoder = SemMapEncoder_with_PosEncodings(model_config.SEM_MAP_TRANSFORMER).to(device=device)\n",
    "# sem = observations_batch['ego_sem_map']\n",
    "\n",
    "print()\n",
    "sem = batch['ego_sem_map'].unsqueeze(1)\n",
    "sem = F.interpolate(sem, size=(20, 20), mode='nearest')\n",
    "\n",
    "print(sem.shape)\n",
    "s_t = sem_map_encoder(sem.squeeze(0), w_t, None, enc_mask)\n",
    "print(s_t.shape)\n",
    "\n",
    "sem_pooler = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool1d((16))\n",
    ").to(device)\n",
    "\n",
    "\n",
    "s_t = sem_pooler(s_t.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "# print(s_t.shape)\n",
    "\n",
    "rgb_ds = torch.cat((i_t, s_t), dim=-1)\n",
    "\n",
    "print(rgb_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooler = nn.AdaptiveAvgPool1d(16).to(device)\n",
    "sem_out = pooler(s_t.permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "print(sem_out.shape)\n",
    "rgb_ds = torch.cat((i_t, sem_out), dim=-1)\n",
    "\n",
    "print(rgb_ds.shape)\n",
    "\n",
    "fc = torch.nn.Linear(i_t.shape[2]+sem_out.shape[2], 512).to(device)\n",
    "\n",
    "rgb_ds = F.relu(fc(rgb_ds))\n",
    "\n",
    "\n",
    "print(rgb_ds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch = observations_batch\n",
    "\n",
    "def semantic_to_image(semantic_obs, num_colors):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "    semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "    semantic_img.putpalette(d3_40_colors_rgb[:num_colors].flatten())\n",
    "    semantic_img.putdata((semantic_obs.flatten() % num_colors).astype(np.uint8))\n",
    "    semantic_img = semantic_img.convert(\"RGB\")\n",
    "    return np.array(semantic_img)\n",
    "\n",
    "\n",
    "# def downsampling(x, size=None, scale_factor=None, mode='bilinear'):\n",
    "#     # define size if user has specified scale_factor\n",
    "#     if size is None: size = (int(scale_factor*x.size(2)), int(scale_factor*x.size(3)))\n",
    "#     # create coordinates\n",
    "#     h = torch.arange(0,size[0]) / (size[0]-1) * 2 - 1\n",
    "#     w = torch.arange(0,size[1]) / (size[1]-1) * 2 - 1\n",
    "#     # create grid\n",
    "#     print(h.shape)\n",
    "#     print(w.shape)\n",
    "#     grid = torch.zeros(size[0],size[1],2)\n",
    "#     grid[:,:,0] = w.unsqueeze(0).repeat(size[0],1)\n",
    "#     grid[:,:,1] = h.unsqueeze(0).repeat(size[1],1).transpose(0,1)\n",
    "#     # expand to match batch size\n",
    "#     grid = grid.unsqueeze(0).repeat(x.size(0),1,1,1)\n",
    "#     if x.is_cuda: grid = grid.cuda()\n",
    "#     # do sampling\n",
    "#     return F.grid_sample(x, grid, mode=mode)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "image = observations_batch['ego_sem_map'][140].unsqueeze(0).unsqueeze(0)\n",
    "# image = batch['ego_sem_map'].unsqueeze(0)\n",
    "\n",
    "print(batch['ego_sem_map'].shape)\n",
    "print(batch['ego_sem_map'].unsqueeze(0).shape)\n",
    "print(batch['ego_sem_map'].unsqueeze(1).shape)\n",
    "\n",
    "# image_small = downsampling(image, size=[20,20])\n",
    "# print(image_small.shape)\n",
    "# Create grid\n",
    "out_size = 20\n",
    "x = torch.linspace(-1, 1, out_size).view(-1, 1).repeat(1, out_size)\n",
    "y = torch.linspace(-1, 1, out_size).repeat(out_size, 1)\n",
    "grid = torch.cat((x.unsqueeze(2), y.unsqueeze(2)), 2).to(device)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.matshow(grid[:,:,0].cpu().numpy())\n",
    "\n",
    "# plt.figure()\n",
    "# plt.matshow(grid[:,:,1].cpu().numpy())\n",
    "\n",
    "# grid.unsqueeze_(0)\n",
    "\n",
    "\n",
    "\n",
    "grid = grid.unsqueeze_(0).repeat(2,1,1,1)\n",
    "\n",
    "print(\"image\", image.shape)\n",
    "\n",
    "# image_small = F.grid_sample(image, grid, mode='nearest', align_corners=True)\n",
    "image_small = F.interpolate(image, size=(15, 15), mode='nearest')\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "ego_map= image_small.squeeze(0).squeeze(0)\n",
    "semantic= image.squeeze(0).squeeze(0)\n",
    "\n",
    "# print(ego_map.shape)\n",
    "# print(semantic.shape)\n",
    "\n",
    "# print(semantic.cpu().numpy())\n",
    "# print(ego_map)\n",
    "\n",
    "# print(image_small)\n",
    "\n",
    "object2idx = envs.call_at(0,'get_object2idx')\n",
    "semantic_obs = object2idx[semantic.cpu().numpy().astype(np.int)] \n",
    "\n",
    "ego_map = object2idx[ego_map.cpu().numpy().astype(np.int)]\n",
    "\n",
    "semantic_img = semantic_to_image(semantic_obs, 40)\n",
    "ego_img= semantic_to_image(ego_map, 40)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(semantic_img)\n",
    "plt.figure()\n",
    "plt.imshow(ego_img)\n",
    "\n",
    "print(\"ego image\", ego_img.shape)\n",
    "\n",
    "# plt.imshow(image.squeeze(0).squeeze(0).cpu().numpy())\n",
    "# plt.imtshow(image_small.squeeze(0).squeeze(0).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "## Single prev action\n",
    "prev_action_list=[]\n",
    "prev_actions = torch.zeros(\n",
    "    config.NUM_PROCESSES, 1, device=device, dtype=torch.long\n",
    ")\n",
    "not_done_masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "if config.NUM_PROCESSES==1:\n",
    "    prev_actions = prev_actions.view(-1)\n",
    "    not_done_masks = not_done_masks.view(-1)\n",
    "\n",
    "\n",
    "## Batch prev actions\n",
    "# prev_actions = prev_actions_batch\n",
    "# batch_size = 5\n",
    "# N = int(prev_actions_batch.size(0)/batch_size)\n",
    "\n",
    "# prev_actions = prev_actions.view(batch_size, N).to(device)\n",
    "# not_done_masks = not_done_masks.view(batch_size, N).to(device)\n",
    "\n",
    "# prev_actions = prev_actions[:,:3]\n",
    "# not_done_masks = not_done_masks[:,:3]\n",
    "\n",
    "num_embeddings = envs.action_spaces[0].n\n",
    "embedding = nn.Embedding(num_embeddings+1, 100, padding_idx= 1).to(device)\n",
    "\n",
    "prev_actions_embedding = embedding(((prev_actions.float()+1)*not_done_masks).long())\n",
    "\n",
    "print(prev_actions_embedding.shape)\n",
    "\n",
    "prev_action_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([rgb_ds, prev_actions_embedding], dim=1)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "state_compress = nn.Sequential(\n",
    "    nn.Linear(\n",
    "        model_config.TRANSFORMER.output_size*2 + embedding.embedding_dim,\n",
    "        model_config.TRANSFORMER.output_size,\n",
    "    ),\n",
    "    nn.ReLU(True),\n",
    ").to(device)\n",
    "\n",
    "\n",
    "x = state_compress(x)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid RNN-Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "torch.Size([2, 5, 512])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# x=rgb_ds\n",
    "# print(x.shape)\n",
    "from vlnce_baselines.models.decoder.hybrid_rnn_decoder import HybridRNNDecoder\n",
    "model_config = config.MODEL\n",
    "state_decoder = HybridRNNDecoder(\n",
    "    model_config.TRANSFORMER.output_size,\n",
    "    hidden_size=512, \n",
    "    num_layers=1,\n",
    "    rnn_type=model_config.HYBRID_STATE_DECODER.rnn_type,\n",
    ")\n",
    "\n",
    "recurrent_hidden_states = torch.zeros(\n",
    "    state_decoder.num_recurrent_layers,\n",
    "    T,\n",
    "    512,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "masked= recurrent_hidden_states[0,:]*ma[:,0].unsqueeze(1).unsqueeze(0).to(device)\n",
    "\n",
    "print(masked)\n",
    "\n",
    "print(recurrent_hidden_states.shape)\n",
    "print(state_decoder.num_recurrent_layers)\n",
    "\n",
    "\n",
    "# not_done_masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "# out, rnn_hidden_states = state_decoder(x, recurrent_hidden_states, not_done_masks)\n",
    "\n",
    "# print(out.shape)\n",
    "# print(rnn_hidden_states.shape)\n",
    "\n",
    "# # num_recurrent_layers = state_decoder.num_recurrent_layers\n",
    "\n",
    "# # def unpack_hidden(hidden_states):\n",
    "# #         hidden_states = (\n",
    "# #             hidden_states[0 : num_recurrent_layers],\n",
    "# #             hidden_states[num_recurrent_layers :],\n",
    "# #         )\n",
    "\n",
    "# #     return hidden_states\n",
    "\n",
    "# # hidden_states = unpack_hidden(recurrent_hidden_states)\n",
    "# # x, hidden_states = rnn(x,mask_hidden(hidden_states, masks.unsqueeze(0))\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "spatial_dim = int(np.sqrt(out.shape[1]))\n",
    "out_1 = out.view(out.shape[0], spatial_dim, spatial_dim, -1).permute(0,3,1,2)\n",
    "\n",
    "print(out_1.shape)\n",
    "out_pos_embed =  position_embedding_2d(out.view(out.shape[0], spatial_dim, spatial_dim, -1).permute(0,3,1,2))\n",
    "out_pos_embed = out_pos_embed.flatten(2).permute(0, 2, 1)\n",
    "print(out_pos_embed.flatten(2).permute(0, 2, 1).shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from vlnce_baselines.models.transformer.transformer import ImageCrossModalEncoder, ImageEncoder_with_PosEncodings, SemMapEncoder_with_PosEncodings\n",
    "\n",
    "image_encoder_trans = ImageEncoder_with_PosEncodings(model_config.IMAGE_CROSS_MODAL_ENCODER).to(device=device)\n",
    "\n",
    "a_t = image_encoder_trans(out, w_t, None, enc_mask, out_pos_embed)\n",
    "\n",
    "print(a_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooler = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool1d((1)),\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "pooled_out = pooler(a_t.permute(0,2,1))\n",
    "\n",
    "print(pooled_out.shape)\n",
    "\n",
    "print(out[-1].shape)\n",
    "\n",
    "out = torch.cat((pooled_out, out[:,-1,:]), dim=1)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(not_done_masks)\n",
    "# print(prev_actions_embedding[90])\n",
    "\n",
    "# prev_action_seq = prev_actions_embedding\n",
    "\n",
    "# prev_action_list = []\n",
    "\n",
    "prev_action_list.append(prev_actions_embedding)\n",
    "prev_action_seq = torch.stack(prev_action_list).transpose(0,1)\n",
    "\n",
    "print(prev_action_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(prev_action_seq.shape)\n",
    "\n",
    "print(len(prev_action_list))\n",
    "\n",
    "print(prev_actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlnce_baselines.models.trasnformer.trasnformer import ActionDecoderTrasnformer\n",
    "\n",
    "seq_len = prev_action_seq.shape[1]\n",
    "\n",
    "prev_action_pad = ((prev_actions+1)*not_done_masks.to(device)).view(T,N)\n",
    "\n",
    "pad= prev_action_pad[:,0:76]\n",
    "\n",
    "prev_action_pad_mask = (pad != 1).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# print(prev_action_pad_mask)\n",
    "mask_self_attention = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).bool().to(device)\n",
    "# print(torch.triu(torch.ones((seq_len, seq_len)), difagonal=1))\n",
    "mask_self_attention = mask_self_attention  # (1, 1, seq_len, seq_len)\n",
    "mask_self_attention = mask_self_attention.gt(0)  # (b_s, 1, seq_len, seq_len)\n",
    "\n",
    "mask = prev_action_pad_mask & ~mask_self_attention\n",
    "\n",
    "print(\"--------------\")\n",
    "print(~mask[1,:,75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlnce_baselines.models.transformer.transformer import ActionDecoderTransformer\n",
    "action_decoder = ActionDecoderTransformer(model_config.ACTION_DECODER_TRANFORMER).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = action_decoder(prev_action_seq, w_t, rgb_ds, enc_att_mask_w=enc_mask, \n",
    "                                    enc_att_mask_i=None, device=device, pos_embed= pos_embed_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlnce_baselines.models.transformer.transformer import ActionDecoder_SemMap\n",
    "action_decoder_sem = ActionDecoder_SemMap(model_config.ACTION_DECODER_TRANFORMER).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sem = action_decoder_sem(prev_action_seq, w_t, i_t,s_t, enc_att_mask_w=mask, \n",
    "                                    enc_att_mask_i=None,enc_att_mask_s=sem_att_mask, device=device, pos_embed= pos_embed_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_sem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "habitat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
