{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check parallel envs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "h=40\n",
    "w=40\n",
    "position_ids = None\n",
    "if position_ids is None:\n",
    "    radius = h\n",
    "    norm_dist = torch.distributions.normal.Normal(0, radius // 4)\n",
    "    x = torch.linspace(-radius, radius, radius + 1)\n",
    "    kern1d = norm_dist.cdf(x)\n",
    "    kern1d = kern1d[1:] - kern1d[:-1]\n",
    "    kern2d = torch.ger(kern1d, kern1d)\n",
    "    position_ids = torch.round(radius**2 * kern2d)\n",
    "    position_ids = position_ids.unsqueeze(0).reshape(1, h, w)\n",
    "    \n",
    "\n",
    "plt.matshow(position_ids.squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "with gzip.open('/data/zirshad/VLNCE-data/data/datasets/R2R_VLNCE_v1_preprocessed/val_seen/val_seen.json.gz') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "num_to_actions={0:'Stop',\n",
    "                1:'Move Forward',\n",
    "                2:'Turn Left',\n",
    "                3:'Turn Right',\n",
    "                4:'Look Up',\n",
    "                5:'Look Down'}\n",
    "\n",
    "# print(data['instruction_vocab']['stoi']\n",
    "# )\n",
    "\n",
    "episode = 0\n",
    "\n",
    "\n",
    "ep_id=164\n",
    "print(data['episodes'][ep_id]\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "myFile = np.genfromtxt('/home/zirshad/SASRA/videos/debug/seq2seq_text_attn/run_7/episode=165-ckpt=0-SPL=1.00attention.csv', delimiter=',')\n",
    "actions = np.genfromtxt('/home/zirshad/SASRA/videos/debug/seq2seq_text_attn/run_7/episode=165-ckpt=0-SPL=1.00action.csv', delimiter=',')\n",
    "\n",
    "actions_name =[]\n",
    "print(myFile[0].shape)\n",
    "print(myFile[0])\n",
    "print(actions.shape)\n",
    "\n",
    "for i in range(len(actions)):\n",
    "    actions_name.append(num_to_actions[actions[i]])\n",
    "    \n",
    "instruction = \"CLS \"+ data['episodes'][ep_id]['instruction']['instruction_text'] + \"EOS\"\n",
    "\n",
    "print(actions_name)\n",
    "print(actions)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "cax = ax.matshow(myFile, cmap='bone')\n",
    "fig.colorbar(cax)\n",
    "fig.set_size_inches(20, 20)\n",
    "plt.xticks(np.arange(0, myFile.shape[1], 1.0))\n",
    "plt.yticks(np.arange(0, myFile.shape[0], 1.0))\n",
    "ax.set_xticklabels(instruction.split(\" \"), rotation=90)\n",
    "ax.set_yticklabels(actions_name)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-05 21:30:05,661 Initializing dataset VLN-CE-v1\n",
      "2020-08-05 21:30:06,591 [construct_envs] Using GPU ID 1\n"
     ]
    }
   ],
   "source": [
    "from vlnce_baselines.common.env_utils import construct_envs\n",
    "from habitat import Config\n",
    "from vlnce_baselines.config.default import get_config\n",
    "from habitat_baselines.common.environments import get_env_class\n",
    "\n",
    "#Import config and perform some manipulation\n",
    "exp_config ='vlnce_baselines/config/paper_configs/seq2seq_sem_attn.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "split = config.TASK_CONFIG.DATASET.SPLIT\n",
    "config.defrost()\n",
    "config.TASK_CONFIG.TASK.NDTW.SPLIT = split\n",
    "config.TASK_CONFIG.TASK.SDTW.SPLIT = split\n",
    "\n",
    "# if doing teacher forcing, don't switch the scene until it is complete\n",
    "if config.DAGGER.P == 1.0:\n",
    "    config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS = (\n",
    "        -1\n",
    "    )\n",
    "config.freeze()\n",
    "envs = construct_envs(config, get_env_class(config.ENV_NAME))\n",
    "obs=envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8979\n",
      "rgb\n",
      "depth\n",
      "semantic\n",
      "instruction\n",
      "vln_oracle_action_sensor\n",
      "progress\n",
      "heading\n",
      "ego_sem_map\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vlnce_baselines.common.utils import transform_obs\n",
    "from habitat_baselines.common.utils import batch_obs\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(envs.num_envs)\n",
    "print(envs.current_episodes()[0].episode_id)\n",
    "observations = envs.reset()\n",
    "# observations[0]['instruction_text'] = 'Go to kitchen'\n",
    "\n",
    "for k,v in observations[0].items():\n",
    "    print(k)\n",
    "# print(observations[0])\n",
    "\n",
    "observations = transform_obs(\n",
    "    observations, config.TASK_CONFIG.TASK.INSTRUCTION_SENSOR_UUID, is_bert=True\n",
    ")\n",
    "\n",
    "# # print(observations)\n",
    "batch = batch_obs(observations, device=device)\n",
    "\n",
    "# print(batch['instruction_batch'])\n",
    "\n",
    "\n",
    "# print (batch[\"instruction_batch\"].shape)\n",
    "# lengths = (batch['instruction_batch'] != 0.0).long().sum(dim=1)\n",
    "# print(lengths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "print(batch['instruction'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Dataset, pytorch Dataloader, padding and lmdb load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import config and perform some manipulation\n",
    "from vlnce_baselines.config.default import get_config\n",
    "exp_config ='vlnce_baselines/config/paper_configs/seq2seq_sem_attn.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "split = config.TASK_CONFIG.DATASET.SPLIT\n",
    "config.defrost()\n",
    "config.TASK_CONFIG.TASK.NDTW.SPLIT = split\n",
    "config.TASK_CONFIG.TASK.SDTW.SPLIT = split\n",
    "\n",
    "# if doing teacher forcing, don't switch the scene until it is complete\n",
    "if config.DAGGER.P == 1.0:\n",
    "    config.TASK_CONFIG.ENVIRONMENT.ITERATOR_OPTIONS.MAX_SCENE_REPEAT_STEPS = (\n",
    "        -1\n",
    "    )\n",
    "config.freeze()\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import lmdb\n",
    "import msgpack_numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "class ObservationsDict(dict):\n",
    "    def pin_memory(self):\n",
    "        for k, v in self.items():\n",
    "            self[k] = v.pin_memory()\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Each sample in batch: (\n",
    "            obs,\n",
    "            prev_actions,\n",
    "            oracle_actions,\n",
    "            inflec_weight,\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    def _pad_helper(t, max_len, fill_val=0):\n",
    "        pad_amount = max_len - t.size(0)\n",
    "        if pad_amount == 0:\n",
    "            return t\n",
    "        \n",
    "        pad = torch.full_like(t[0:1], fill_val).expand(pad_amount, *t.size()[1:])\n",
    "        return torch.cat([t, pad], dim=0)\n",
    "\n",
    "    transposed = list(zip(*batch))\n",
    "\n",
    "    observations_batch = list(transposed[0])\n",
    "    prev_actions_batch = list(transposed[1])\n",
    "    corrected_actions_batch = list(transposed[2])\n",
    "    N = len(corrected_actions_batch)\n",
    "    weights_batch = list(transposed[3])\n",
    "    B = len(prev_actions_batch)\n",
    "    new_observations_batch = defaultdict(list)\n",
    "    for sensor in observations_batch[0]:\n",
    "        if sensor == 'instruction':\n",
    "            for bid in range(N):\n",
    "                new_observations_batch[sensor].append(observations_batch[bid][sensor])\n",
    "        else: \n",
    "            for bid in range(B):\n",
    "                new_observations_batch[sensor].append(observations_batch[bid][sensor])\n",
    "\n",
    "    observations_batch = new_observations_batch\n",
    "\n",
    "    max_traj_len = max(ele.size(0) for ele in prev_actions_batch)\n",
    "#     obs_lengths=[]\n",
    "    for bid in range(B):\n",
    "        for sensor in observations_batch:\n",
    "            if sensor == 'instruction':\n",
    "                continue\n",
    "            observations_batch[sensor][bid] = _pad_helper(\n",
    "                observations_batch[sensor][bid], max_traj_len, fill_val=0.0\n",
    "            )\n",
    "#         obs_lengths.append(prev_actions_batch[bid].shape[0])\n",
    "        prev_actions_batch[bid] = _pad_helper(prev_actions_batch[bid], max_traj_len)\n",
    "        corrected_actions_batch[bid] = _pad_helper(\n",
    "            corrected_actions_batch[bid], max_traj_len, fill_val=-1.0\n",
    "        )\n",
    "        weights_batch[bid] = _pad_helper(weights_batch[bid], max_traj_len)\n",
    "\n",
    "    for sensor in observations_batch:\n",
    "        observations_batch[sensor] = torch.stack(observations_batch[sensor], dim=1)\n",
    "        observations_batch[sensor] = observations_batch[sensor].transpose(1,0)\n",
    "        observations_batch[sensor] = observations_batch[sensor].contiguous().view(\n",
    "            -1, *observations_batch[sensor].size()[2:]\n",
    "        )\n",
    "        # if sensor == 'instruction':\n",
    "        #     observations_batch[sensor] = observations_batch[sensor].contiguous().view(\n",
    "        #    -1, *observations_batch[sensor].size()[2:]\n",
    "        #  )\n",
    "\n",
    "\n",
    "    prev_actions_batch = torch.stack(prev_actions_batch, dim=1)\n",
    "    corrected_actions_batch = torch.stack(corrected_actions_batch, dim=1)\n",
    "    weights_batch = torch.stack(weights_batch, dim=1)\n",
    "    not_done_masks = torch.ones_like(corrected_actions_batch, dtype=torch.float)\n",
    "    not_done_masks[0] = 0\n",
    "    \n",
    "    prev_actions_batch = prev_actions_batch.transpose(1,0)\n",
    "    not_done_masks = not_done_masks.transpose(1,0)\n",
    "    corrected_actions_batch = corrected_actions_batch.transpose(1,0)\n",
    "    weights_batch = weights_batch.transpose(1,0)\n",
    "    \n",
    "    observations_batch = ObservationsDict(observations_batch)\n",
    "\n",
    "    return (\n",
    "        observations_batch,\n",
    "        prev_actions_batch.contiguous().view(-1, 1),\n",
    "        not_done_masks.contiguous().view(-1, 1),\n",
    "        corrected_actions_batch,\n",
    "        weights_batch,\n",
    "    )\n",
    "    # return (\n",
    "    #     observations_batch,\n",
    "    #     prev_actions_batch,\n",
    "    #     not_done_masks,\n",
    "    #     corrected_actions_batch,\n",
    "    #     weights_batch,\n",
    "    # )\n",
    "\n",
    "\n",
    "def _block_shuffle(lst, block_size):\n",
    "    blocks = [lst[i : i + block_size] for i in range(0, len(lst), block_size)]\n",
    "    random.shuffle(blocks)\n",
    "\n",
    "    return [ele for block in blocks for ele in block]\n",
    "\n",
    "\n",
    "class IWTrajectoryDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lmdb_features_dir,\n",
    "        use_iw,\n",
    "        inflection_weight_coef=1.0,\n",
    "        lmdb_map_size=1e9,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lmdb_features_dir = lmdb_features_dir\n",
    "        self.lmdb_map_size = lmdb_map_size\n",
    "        self.preload_size = batch_size * 100\n",
    "        self._preload = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if use_iw:\n",
    "            self.inflec_weights = torch.tensor([1.0, inflection_weight_coef])\n",
    "        else:\n",
    "            self.inflec_weights = torch.tensor([1.0, 1.0])\n",
    "\n",
    "        with lmdb.open(\n",
    "            self.lmdb_features_dir,\n",
    "            map_size=int(self.lmdb_map_size),\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "        ) as lmdb_env:\n",
    "            self.length = lmdb_env.stat()[\"entries\"]\n",
    "\n",
    "    def _load_next(self):\n",
    "        if len(self._preload) == 0:\n",
    "            if len(self.load_ordering) == 0:\n",
    "                raise StopIteration\n",
    "\n",
    "            new_preload = []\n",
    "            lengths = []\n",
    "            with lmdb.open(\n",
    "                self.lmdb_features_dir,\n",
    "                map_size=int(self.lmdb_map_size),\n",
    "                readonly=True,\n",
    "                lock=False,\n",
    "            ) as lmdb_env, lmdb_env.begin(buffers=True) as txn:\n",
    "                for _ in range(self.preload_size):\n",
    "                    if len(self.load_ordering) == 0:\n",
    "                        break\n",
    "\n",
    "                    new_preload.append(\n",
    "                        msgpack_numpy.unpackb(\n",
    "                            txn.get(str(self.load_ordering.pop()).encode()), raw=False\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    lengths.append(len(new_preload[-1][0]))\n",
    "\n",
    "            sort_priority = list(range(len(lengths)))\n",
    "            random.shuffle(sort_priority)\n",
    "\n",
    "            sorted_ordering = list(range(len(lengths)))\n",
    "            sorted_ordering.sort(key=lambda k: (lengths[k], sort_priority[k]))\n",
    "\n",
    "            for idx in _block_shuffle(sorted_ordering, self.batch_size):\n",
    "                self._preload.append(new_preload[idx])\n",
    "\n",
    "        return self._preload.pop()\n",
    "\n",
    "    def __next__(self):\n",
    "        obs, prev_actions, oracle_actions= self._load_next()\n",
    "        instruction_batch = obs['instruction'][0]\n",
    "        instruction_batch = np.expand_dims(instruction_batch, axis=0)\n",
    "        obs['instruction'] = instruction_batch\n",
    "        for k, v in obs.items():\n",
    "            obs[k] = torch.from_numpy(v)\n",
    "\n",
    "        prev_actions = torch.from_numpy(prev_actions)\n",
    "        \n",
    "        \n",
    "        oracle_actions = torch.from_numpy(oracle_actions)\n",
    "\n",
    "        inflections = torch.cat(\n",
    "            [\n",
    "                torch.tensor([1], dtype=torch.long),\n",
    "                (oracle_actions[1:] != oracle_actions[:-1]).long(),\n",
    "            ]\n",
    "        )\n",
    "        return (obs, prev_actions, oracle_actions, self.inflec_weights[inflections])\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            start = 0\n",
    "            end = self.length\n",
    "        else:\n",
    "            per_worker = int(np.ceil(self.length / worker_info.num_workers))\n",
    "\n",
    "            start = per_worker * worker_info.id\n",
    "            end = min(start + per_worker, self.length)\n",
    "\n",
    "        # Reverse so we can use .pop()\n",
    "        self.load_ordering = list(\n",
    "            reversed(_block_shuffle(list(range(start, end)), self.preload_size))\n",
    "        )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmdb_features_dir = 'data/trajectories_dirs/seq2seq/trajectories.lmdb'\n",
    "lmdb_features_dir = '/data/zirshad/VLNCE-data/data/trajectory_dir/seq2seq_sem_attn/trajectories.lmdb'\n",
    "\n",
    "USE_IW= False\n",
    "\n",
    "dataset = IWTrajectoryDataset(\n",
    "    lmdb_features_dir,\n",
    "    USE_IW,\n",
    "    inflection_weight_coef=1.0,\n",
    "    lmdb_map_size=1e9,\n",
    "    batch_size=6,\n",
    ")\n",
    "diter = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=6,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    "    drop_last=True,  # drop last batch if smaller\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "observations_batch,prev_actions_batch,not_done_masks,corrected_actions_batch,weights_batch = next(iter(diter))\n",
    "# print(observations_batch['depth_features'].shape)\n",
    "# print(prev_actions_batch.shape)\n",
    "observations_batch = {\n",
    "    k: v.to(device=device, non_blocking=True)\n",
    "    for k, v in observations_batch.items()\n",
    "}\n",
    "\n",
    "# rgb_out_dim = observations_batch['rgb'].shape[3]\n",
    "# rgb_out_dim = observations_batch['depth'].shape[3]\n",
    "\n",
    "# prev_actions     = prev_actions.view(T,N)\n",
    "# masks            = masks.view(T,N)\n",
    "# rgb_embedding    = rgb_embedding.view(self.batch_size, max_len, -1, rgb_out_dim, rgb_out_dim)\n",
    "# depth_embedding  = depth_embedding.view(self.batch_size, max_len, -1, depth_out_dim, depth_out_dim)\n",
    "\n",
    "for k, v in observations_batch.items():\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations_batch['progress'].dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dataset.length)\n",
    "print(observations_batch['instruction'].shape)\n",
    "# print(observations_batch['ego_sem_map'].shape)\n",
    "print(len(prev_actions_batch))\n",
    "\n",
    "max_traj_len = max(ele.size(0) for ele in prev_actions_batch)\n",
    "print(corrected_actions_batch.size())\n",
    "print(prev_actions_batch.size())\n",
    "\n",
    "print(corrected_actions_batch.shape)\n",
    "print(weights_batch.shape)\n",
    "\n",
    "print(observations_batch['rgb_features'].shape)\n",
    "print(observations_batch['depth_features'].shape)\n",
    "\n",
    "T,N = corrected_actions_batch.shape\n",
    "\n",
    "print(not_done_masks.shape)\n",
    "\n",
    "# print(prev_actions_batch.view(T,N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corrected_actions_batch)\n",
    "# print(observations_batch['instruction'][1])\n",
    "\n",
    "\n",
    "\n",
    "print(observations_batch['rgb_features'].shape)\n",
    "print(observations_batch['depth_features'].shape)\n",
    "\n",
    "lengths = (observations_batch['instruction'] != 0.0).long().sum(dim=1)\n",
    "\n",
    "print(prev_actions_batch.view(T,N).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Actor Critic Agent /Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def load_checkpoint(checkpoint_path, *args, **kwargs) -> Dict:\n",
    "    r\"\"\"Load checkpoint of specified path as a dict.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path: path of target checkpoint\n",
    "        *args: additional positional args\n",
    "        **kwargs: additional keyword args\n",
    "\n",
    "    Returns:\n",
    "        dict containing checkpoint info\n",
    "    \"\"\"\n",
    "    return torch.load(checkpoint_path, *args, **kwargs)\n",
    "\n",
    "ckpt_path = '/data/zirshad/VLNCE-data/data/checkpoints/seq2seq_text_attn/ckpt.24.pth'\n",
    "\n",
    "# from vlnce_baselines.models.cma_policy import CMAPolicy\n",
    "# from vlnce_baselines.models.seq2seq_text_attn import Seq2Seq_Lang_Attn\n",
    "from vlnce_baselines.models.transformer_policy import TransformerPolicy\n",
    "import torch\n",
    "\n",
    "load_from_ckpt = False\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "model_config = config.MODEL\n",
    "\n",
    "model_config.defrost()\n",
    "model_config.TORCH_GPU_ID = config.TORCH_GPU_ID\n",
    "model_config.freeze()\n",
    "actor_critic = TransformerPolicy(\n",
    "                observation_space=envs.observation_spaces[0],\n",
    "                action_space=envs.action_spaces[0],\n",
    "                model_config=model_config,\n",
    "                num_processes = config.NUM_PROCESSES,\n",
    "                batch_size=config.DAGGER.BATCH_SIZE,\n",
    "            )\n",
    "actor_critic.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(actor_critic.parameters(), \n",
    "                                         lr=config.MODEL.TRANSFORMER.lr, \n",
    "                                         betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, \n",
    "                                                               patience=config.MODEL.TRANSFORMER.scheduler_patience, \n",
    "                                                               verbose=True, min_lr=1e-6)\n",
    "\n",
    "if load_from_ckpt:\n",
    "    ckpt_dict = load_checkpoint(ckpt_path, map_location=\"cpu\")\n",
    "    actor_critic.load_state_dict(ckpt_dict[\"state_dict\"])\n",
    "    \n",
    "# if config.DAGGER.PRELOAD_LMDB_FEATURES:\n",
    "#     envs.close()\n",
    "#     del envs\n",
    "#     envs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Individual Architecture Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zirshad/.conda/envs/habitat/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded shape: torch.Size([1, 200, 768])\n",
      "hidden torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 21, 256])\n",
      "tensor([[ 0.0824, -0.0214, -0.1457,  ..., -0.0278,  0.0441, -0.0408],\n",
      "        [ 0.1544,  0.2265,  0.2395,  ..., -0.2030,  0.0568,  0.1381],\n",
      "        [-0.1056,  0.2373,  0.2702,  ..., -0.1626, -0.0258, -0.1785],\n",
      "        ...,\n",
      "        [-0.1240, -0.1151,  0.2143,  ...,  0.0139,  0.2472,  0.0384],\n",
      "        [ 0.0012, -0.1161,  0.0758,  ...,  0.1469,  0.1748,  0.0948],\n",
      "        [-0.1288, -0.0899,  0.0236,  ..., -0.0771,  0.1580, -0.0919]],\n",
      "       device='cuda:1', grad_fn=<SelectBackward>)\n",
      "tensor([[[-0.0685,  0.1152,  0.0360, -0.1138,  0.0303,  0.0773, -0.0295,\n",
      "          -0.0534, -0.1413,  0.0747, -0.0195, -0.1507,  0.1109,  0.0946,\n",
      "          -0.0670, -0.0131, -0.1124,  0.0644, -0.1639,  0.0289,  0.0507,\n",
      "           0.0817, -0.1222,  0.0652,  0.0090, -0.0028,  0.0868,  0.0397,\n",
      "          -0.1426, -0.0007,  0.1578,  0.0430, -0.0605, -0.1463,  0.1091,\n",
      "          -0.0285,  0.0697, -0.0436,  0.1908, -0.0031, -0.2223,  0.3170,\n",
      "          -0.0178,  0.1270,  0.0919,  0.1046, -0.0545,  0.0626,  0.1908,\n",
      "          -0.0555,  0.1057, -0.1758,  0.0474, -0.0983, -0.0199, -0.0422,\n",
      "          -0.0401,  0.0719,  0.0776, -0.1096,  0.1596, -0.0083, -0.0254,\n",
      "           0.0211,  0.1222, -0.0263,  0.0989,  0.1317,  0.0708, -0.1335,\n",
      "          -0.0481, -0.0257,  0.1648, -0.0690,  0.0487, -0.1818,  0.1546,\n",
      "          -0.0321,  0.0306, -0.2123, -0.0898, -0.1501, -0.0164,  0.1684,\n",
      "           0.0588,  0.1564, -0.0627, -0.1498,  0.0968, -0.0686,  0.0654,\n",
      "           0.0079,  0.2671,  0.3352,  0.0393, -0.0359,  0.0802, -0.0523,\n",
      "           0.0430,  0.0905,  0.1337,  0.0418,  0.0209, -0.2151,  0.1204,\n",
      "           0.0560, -0.0033,  0.1024,  0.0456,  0.0806,  0.0554, -0.0335,\n",
      "          -0.1260, -0.0940, -0.1659,  0.0596,  0.0105, -0.1028, -0.0299,\n",
      "          -0.0704,  0.1152,  0.1246,  0.0972,  0.0020, -0.1924, -0.0356,\n",
      "          -0.0579,  0.0530,  0.0594,  0.1817,  0.0704, -0.0216,  0.1248,\n",
      "          -0.2307,  0.1297,  0.0342,  0.1417,  0.1699, -0.0702, -0.0154,\n",
      "           0.0744, -0.0035,  0.0058, -0.0607, -0.1593, -0.1233, -0.0283,\n",
      "          -0.0090,  0.0045, -0.1020,  0.0303,  0.1222, -0.0849, -0.0078,\n",
      "           0.1992, -0.0109, -0.0351, -0.1034,  0.2003, -0.0722, -0.1903,\n",
      "           0.1326, -0.1213, -0.0205,  0.0536, -0.0651, -0.0572, -0.1003,\n",
      "           0.1693, -0.1112, -0.0981, -0.0337,  0.0735,  0.0250,  0.1135,\n",
      "           0.0986,  0.0327, -0.1539, -0.2379, -0.0392,  0.0828,  0.2390,\n",
      "          -0.0301, -0.0417, -0.1608,  0.0905,  0.1480, -0.1467,  0.0149,\n",
      "           0.1234, -0.0782,  0.0854,  0.2546,  0.1579,  0.0620, -0.0520,\n",
      "           0.0462, -0.1141,  0.1200, -0.0632,  0.0288, -0.0881, -0.0092,\n",
      "          -0.0663, -0.1151, -0.0718, -0.0444, -0.0512, -0.0472, -0.0788,\n",
      "           0.0661, -0.0330,  0.2630, -0.2134,  0.0228,  0.2103, -0.2085,\n",
      "           0.0363,  0.1183,  0.1660,  0.0424,  0.0076, -0.0891, -0.1508,\n",
      "           0.1043, -0.0910,  0.0393,  0.0687, -0.0976, -0.1432, -0.2441,\n",
      "           0.0919,  0.1409,  0.1165,  0.1588,  0.0018, -0.0394,  0.0260,\n",
      "          -0.0960, -0.1875, -0.0389, -0.0989, -0.0459,  0.1394,  0.1867,\n",
      "          -0.0223,  0.1053,  0.0158,  0.0009, -0.2485, -0.0800,  0.0445,\n",
      "           0.1192, -0.1314, -0.0713,  0.0297]]], device='cuda:1',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "observations_batch = batch\n",
    "device = (\n",
    "    torch.device(\"cuda\", config.TORCH_GPU_ID)\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "observations_batch = {\n",
    "    k: v.to(device=device, non_blocking=True)\n",
    "    for k, v in observations_batch.items()\n",
    "}\n",
    "\n",
    "model_config = config.MODEL\n",
    "from vlnce_baselines.models.encoders.language_encoder import LanguageEncoder\n",
    "instruction_encoder = LanguageEncoder(model_config.INSTRUCTION_ENCODER, device).to(device)\n",
    "encoder_output,encoder_hidden = instruction_encoder(observations_batch)\n",
    "\n",
    "print(encoder_hidden[0].shape)\n",
    "print(encoder_output.shape)\n",
    "# print(encoder_output.permute(1,0,2)[27][0])\n",
    "# print(encoder_output.permute(1,0,2)[22][1])\n",
    "# print(encoder_hidden[0])\n",
    "\n",
    "print(encoder_output[-1])\n",
    "\n",
    "print(encoder_hidden[0])\n",
    "\n",
    "bidir = model_config.INSTRUCTION_ENCODER.bidirectional\n",
    "\n",
    "num_directions = 2 if bidir else 1\n",
    "encoder2decoder = nn.Linear(model_config.INSTRUCTION_ENCODER.hidden_size * num_directions, model_config.INSTRUCTION_ENCODER.hidden_size * num_directions).to(device)\n",
    "print(encoder_hidden[-1].shape)\n",
    "h_t = torch.tanh(encoder2decoder(encoder_hidden[0][-1]))\n",
    "c_t = encoder_hidden[1]\n",
    "h_t = h_t.unsqueeze(0)\n",
    "\n",
    "encoder_hidden = (h_t, c_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 1, 256])\n",
      "torch.Size([1, 21, 256])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_hidden[0].shape)\n",
    "print(h_t.shape)\n",
    "print(c_t.shape)\n",
    "\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB + Depth Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-05 21:32:29,394 Overwriting CNN input size of depth: (256, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 192, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "model_config = config.MODEL\n",
    "def create_mask(batchsize, max_length, length,device):\n",
    "    \"\"\"Given the length create a mask given a padded tensor\"\"\"\n",
    "    tensor_mask = torch.zeros(batchsize, max_length, dtype = torch.bool)\n",
    "    for idx, row in enumerate(tensor_mask):\n",
    "        row[:length[idx]] = 1\n",
    "    tensor_mask.unsqueeze_(-1)\n",
    "    return tensor_mask.to(device)\n",
    "\n",
    "from vlnce_baselines.models.encoders.resnet_encoders import (\n",
    "    TorchVisionResNet50,\n",
    "    VlnResnetDepthEncoder,\n",
    ")\n",
    "observation_space=envs.observation_spaces[0]\n",
    "action_space=envs.action_spaces[0]\n",
    "depth_encoder = VlnResnetDepthEncoder(\n",
    "                observation_space,\n",
    "                output_size=model_config.DEPTH_ENCODER.output_size,\n",
    "                checkpoint=model_config.DEPTH_ENCODER.ddppo_checkpoint,\n",
    "                backbone=model_config.DEPTH_ENCODER.backbone,\n",
    "                spatial_output=True,\n",
    "            )\n",
    "rgb_encoder = TorchVisionResNet50(\n",
    "                observation_space, model_config.RGB_ENCODER.output_size, \n",
    "                model_config.RGB_ENCODER.resnet_output_size,\n",
    "                device, \n",
    "                spatial_output=True,\n",
    "            )\n",
    "\n",
    "rgb_encoder = rgb_encoder.to(device)\n",
    "depth_encoder = depth_encoder.to(device)\n",
    "\n",
    "depth_embedding = depth_encoder(observations_batch)\n",
    "print(depth_embedding.shape)\n",
    "rgb_embedding = rgb_encoder(observations_batch)\n",
    "# x = torch.cat([depth_embedding, rgb_embedding], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2112, 4, 4])\n",
      "torch.Size([1, 2112, 16])\n",
      "torch.Size([1, 192, 16])\n"
     ]
    }
   ],
   "source": [
    "print(rgb_embedding.shape)\n",
    "\n",
    "depth_embedding = torch.flatten(depth_embedding, 2)\n",
    "rgb_embedding = torch.flatten(rgb_embedding, 2)\n",
    "\n",
    "print(rgb_embedding.shape)\n",
    "print(depth_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB DEPTH PRE-PROCESSING TO GET EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n",
      "512\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "print(rgb_encoder.output_shape[0])\n",
    "print(model_config.RGB_ENCODER.output_size)\n",
    "\n",
    "import numpy as np\n",
    "rgb_linear = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool1d(1),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(\n",
    "                    rgb_encoder.output_shape[0],\n",
    "                    model_config.RGB_ENCODER.output_size,\n",
    "                ),\n",
    "                nn.ReLU(True),\n",
    "            ).to(device)\n",
    "\n",
    "depth_linear = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(\n",
    "                    np.prod(depth_encoder.output_shape),\n",
    "                    model_config.DEPTH_ENCODER.output_size,\n",
    "                ),\n",
    "                nn.ReLU(True),\n",
    "            ).to(device)\n",
    "\n",
    "rgb_in = rgb_linear(rgb_embedding)\n",
    "depth_in = depth_linear(depth_embedding)\n",
    "\n",
    "print(rgb_in.shape)\n",
    "print(depth_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial RNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 32])\n",
      "2\n",
      "rnn hidden states torch.Size([4, 1, 256])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([4, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "prev_actions = torch.zeros(\n",
    "    config.NUM_PROCESSES, 1, device=device, dtype=torch.long\n",
    ")\n",
    "\n",
    "\n",
    "masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "num_actions=envs.action_spaces[0].n\n",
    "prev_action_embedding = nn.Embedding(num_actions + 1, 32).to(device)\n",
    "prev_actions = prev_action_embedding(\n",
    "    ((prev_actions.float() + 1) * masks).long().view(-1)\n",
    ")\n",
    "\n",
    "from habitat_baselines.rl.models.rnn_state_encoder import RNNStateEncoder\n",
    "from habitat_baselines.rl.models.rnn_state_encoder import RNNStateEncoder\n",
    "rnn_input_size = model_config.DEPTH_ENCODER.output_size\n",
    "rnn_input_size += model_config.RGB_ENCODER.output_size\n",
    "rnn_input_size += prev_action_embedding.embedding_dim\n",
    "\n",
    "\n",
    "state_encoder = RNNStateEncoder(\n",
    "    input_size=rnn_input_size,\n",
    "    hidden_size=model_config.STATE_ENCODER.hidden_size,\n",
    "    num_layers=1,\n",
    "    rnn_type=model_config.STATE_ENCODER.rnn_type,\n",
    ").to(device)\n",
    "\n",
    "num_recurrent_layers = state_encoder.num_recurrent_layers + (\n",
    "            state_encoder.num_recurrent_layers\n",
    "        )\n",
    "\n",
    "rnn_hidden_states = torch.ones(\n",
    "    num_recurrent_layers,\n",
    "    config.NUM_PROCESSES,\n",
    "    config.MODEL.STATE_ENCODER.hidden_size,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(num_recurrent_layers)\n",
    "\n",
    "\n",
    "\n",
    "print(prev_actions.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(state_encoder.num_recurrent_layers)\n",
    "\n",
    "print(\"rnn hidden states\",rnn_hidden_states.shape)\n",
    "\n",
    "state_in = torch.cat([rgb_in, depth_in, prev_actions], dim=1)\n",
    "(\n",
    "    state,\n",
    "    rnn_hidden_states[0 : state_encoder.num_recurrent_layers],\n",
    ") = state_encoder(\n",
    "    state_in,\n",
    "    rnn_hidden_states[0 : state_encoder.num_recurrent_layers],\n",
    "    masks,\n",
    ")\n",
    "\n",
    "print(state.shape)\n",
    "\n",
    "print(rnn_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale tensor(0.0884)\n",
      "256\n",
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "scale = torch.tensor(1.0 / ((hidden_size // 2) ** 0.5))\n",
    "\n",
    "print(\"scale\", scale)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "def attn(q, k, v, scale, mask=None):\n",
    "    logits = torch.einsum(\"nc, nci -> ni\", q, k)\n",
    "\n",
    "    if mask is not None:\n",
    "        logits = logits - mask.float() * 1e8\n",
    "\n",
    "    attn = F.softmax(logits * scale, dim=1)\n",
    "\n",
    "    return torch.einsum(\"ni, nci -> nc\", attn, v)\n",
    "\n",
    "instruction_embedding = encoder_output.permute(0,2,1)\n",
    "hidden_size = model_config.STATE_ENCODER.hidden_size\n",
    "\n",
    "print(hidden_size)\n",
    "\n",
    "state_q = torch.nn.Linear(hidden_size, hidden_size // 2).to(device)\n",
    "text_k = torch.nn.Conv1d(\n",
    "    instruction_encoder.output_size, hidden_size // 2, 1\n",
    ").to(device)\n",
    "text_q = nn.Linear(instruction_encoder.output_size, hidden_size // 2)\n",
    "\n",
    "text_state_q = state_q(state)\n",
    "text_state_k = text_k(instruction_embedding)\n",
    "text_mask = (instruction_embedding == 0.0).all(dim=1)\n",
    "text_embedding = attn(\n",
    "    text_state_q, text_state_k, instruction_embedding, scale, text_mask\n",
    ")\n",
    "\n",
    "print(text_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n",
      "torch.Size([1, 640, 16])\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(rgb_encoder.output_shape[0])\n",
    "\n",
    "rgb_kv = nn.Conv1d(\n",
    "    rgb_encoder.output_shape[0],\n",
    "    hidden_size // 2 + model_config.RGB_ENCODER.output_size,\n",
    "    1,\n",
    ").to(device)\n",
    "\n",
    "a= rgb_kv(rgb_embedding)\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "print(hidden_size // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 16])\n"
     ]
    }
   ],
   "source": [
    "rgb_kv = nn.Conv1d(\n",
    "    rgb_encoder.output_shape[0],\n",
    "    hidden_size // 2 + 128,\n",
    "    1,\n",
    ").to(device)\n",
    "\n",
    "depth_kv = nn.Conv1d(\n",
    "    depth_encoder.output_shape[0],\n",
    "    hidden_size // 2 + model_config.DEPTH_ENCODER.output_size,\n",
    "    1,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "rgb_k, rgb_v = torch.split(rgb_kv(rgb_embedding), hidden_size // 2, dim=1\n",
    ")\n",
    "depth_k, depth_v = torch.split(\n",
    "    depth_kv(depth_embedding), hidden_size // 2, dim=1\n",
    ")\n",
    "\n",
    "print(rgb_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(batchsize, max_length, length,device):\n",
    "    \"\"\"Given the length create a mask given a padded tensor\"\"\"\n",
    "    tensor_mask = torch.zeros(batchsize, max_length, dtype = torch.bool)\n",
    "    print(tensor_mask.shape)\n",
    "    for idx, row in enumerate(tensor_mask):\n",
    "        row[:length[idx]] = 1\n",
    "    tensor_mask.unsqueeze_(-1)\n",
    "    return tensor_mask.to(device)\n",
    "\n",
    "# not_done_masks = torch.ones(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "# print(not_done_masks)\n",
    "\n",
    "# not_done_masks[0] = 0\n",
    "\n",
    "# def mask_hidden(hidden_states, masks):\n",
    "#     if isinstance(hidden_states, tuple):\n",
    "#         hidden_states = tuple(v * masks for v in hidden_states)\n",
    "#     else:\n",
    "#         hidden_states = masks * hidden_states\n",
    "\n",
    "#     return hidden_states\n",
    "\n",
    "# decoder_hidden = encoder_hidden\n",
    "# print(decoder_hidden[0])\n",
    "# print(1-not_done_masks)\n",
    "\n",
    "# done_masks = 1- not_done_masks\n",
    "# print(not_done_masks.unsqueeze(0))\n",
    "\n",
    "\n",
    "\n",
    "# hidden_states = mask_hidden(decoder_hidden, done_masks.unsqueeze(0))\n",
    "\n",
    "# print(hidden_states[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "prev_actions = torch.zeros(\n",
    "    config.NUM_PROCESSES, 1, device=device, dtype=torch.long\n",
    ")\n",
    "not_done_masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "not_done_masks[0] = 0\n",
    "\n",
    "# Init the RNN state decoder\n",
    "\n",
    "# observations, prev_actions, not_done_masks, corrected_actions, weights= observations_batch, prev_actions_batch.to(device=device, non_blocking=True), not_done_masks.to(device=device, non_blocking=True), corrected_actions_batch.to(device=device, non_blocking=True), weights_batch.to(device=device, non_blocking=True)\n",
    "# T,N = corrected_actions.size()\n",
    "\n",
    "\n",
    "from vlnce_baselines.models.decoder.attn_decoder import Attn_Decoder,Attn_DecoderSequence\n",
    "rnn_input_size = (model_config.DEPTH_ENCODER.output_size\n",
    "    + model_config.RGB_ENCODER.output_size\n",
    ")\n",
    "state_encoder = Attn_DecoderSequence(\n",
    "    attn_model = 'general',\n",
    "    input_size=rnn_input_size,\n",
    "    hidden_size=model_config.STATE_ENCODER.hidden_size,\n",
    "    num_layers=1,\n",
    "    rnn_type=model_config.STATE_ENCODER.rnn_type,\n",
    ").to(device)\n",
    "\n",
    "#RNN Input Size (Seq Length, Batch Size, Input Size)\n",
    "#RNN Hidden Size (Num_layers, Batch Size, Hidden Size)\n",
    "\n",
    "# rnn_hidden_states = torch.ones(\n",
    "#     state_encoder.num_recurrent_layers,\n",
    "#     N,\n",
    "#     config.MODEL.STATE_ENCODER.hidden_size,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "decoder_hidden = encoder_hidden\n",
    "x = torch.cat([depth_embedding, rgb_embedding], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "num_layers =1\n",
    "rnn_type = model_config.STATE_ENCODER.rnn_type\n",
    "rnn = getattr(nn, rnn_type)(\n",
    "    input_size=rnn_input_size,\n",
    "    hidden_size=model_config.STATE_ENCODER.hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    batch_first = True,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "batch_size = decoder_hidden[0].size(1)\n",
    "\n",
    "N = int(x.size(0)/batch_size)\n",
    "\n",
    "# # unflatten\n",
    "# x = x.view(batch_size, N, -1)\n",
    "\n",
    "# x =x[:,0,:]\n",
    "# x= x.unsqueeze(1)\n",
    "# print(x.shape)\n",
    "\n",
    "# # embed_mask = create_mask(x.shape[0], x.shape[1], obs_lengths, device)\n",
    "# # embed_mask = embed_mask.expand_as(x)\n",
    "# # x = x*embed_mask\n",
    "\n",
    "# # print(x.shape)\n",
    "\n",
    "# # packed_seq = nn.utils.rnn.pack_padded_sequence(x, obs_lengths, batch_first=True, enforce_sorted=False)\n",
    "# output, hidden_states = rnn(x, decoder_hidden)\n",
    "\n",
    "# attn = nn.Linear(model_config.STATE_ENCODER.hidden_size, model_config.STATE_ENCODER.hidden_size, bias=False).to(device)\n",
    "# method ='general'\n",
    "# batch_size, output_len, dimensions = output.size()\n",
    "# max_len = encoder_output.size(1)\n",
    "\n",
    "# if method == \"general\":\n",
    "#     output = output.reshape(batch_size * output_len, dimensions)\n",
    "#     output = attn(output)\n",
    "#     output = output.reshape(batch_size, output_len, dimensions)\n",
    "    \n",
    "# print(output.shape)\n",
    "\n",
    "# print(encoder_output.transpose(1,2).shape)\n",
    "# attention_scores = torch.bmm(output, encoder_output.transpose(1, 2).contiguous())\n",
    "\n",
    "# print(attention_scores.shape)\n",
    "\n",
    "# # print(output.shape)\n",
    "\n",
    "# # print(attention_scores.shape)\n",
    "\n",
    "# # print(attention_scores[1,0,:])\n",
    "\n",
    "# mask = create_mask(encoder_output.shape[0], encoder_output.shape[1], lengths, device)\n",
    "# mask = mask.permute(0,2,1)\n",
    "# mask = mask.expand(encoder_output.shape[0], output_len, encoder_output.shape[1])\n",
    "\n",
    "# # print(mask.shape)\n",
    "\n",
    "# attention_scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "\n",
    "# print(attention_scores)\n",
    "# # print(attention_scores[1,20,:])\n",
    "\n",
    "# attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# print(attention_weights)\n",
    "# attention_weights = attention_weights.view(batch_size, output_len, max_len)\n",
    "\n",
    "# print(attention_weights.shape)\n",
    "# print(encoder_output.shape)\n",
    "\n",
    "# context = attention_weights.bmm(encoder_output)\n",
    "\n",
    "# concat_input = torch.cat((output, context), 2)\n",
    "\n",
    "# concat_input = concat_input.contiguous().view(batch_size*N, -1)  # flatten\n",
    "\n",
    "# # print(concat_input.shape)\n",
    "\n",
    "# # print(context.shape)\n",
    "# # print(attention_weights.shape)\n",
    "# # print(output.shape)\n",
    "\n",
    "# concat = nn.Linear(hidden_size * 2, hidden_size).to(device)\n",
    "# print(concat_input.shape)\n",
    "\n",
    "# print(x.size())\n",
    "# print(decoder_hidden[0].size())\n",
    "x, rnn_hidden_states, attn_weights = state_encoder(x, decoder_hidden, encoder_hidden, encoder_output, not_done_masks, lengths, device)\n",
    "\n",
    "# print(x.shape)\n",
    "# print(attn_weights.shape)\n",
    "\n",
    "# print(attn_weights.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_hidden(decoder_hidden, encoder_hidden, masks):\n",
    "    if isinstance(decoder_hidden, tuple):\n",
    "        print(\"tupleeee\")\n",
    "        done_masks = 1-masks\n",
    "        hidden_states = tuple(v * masks + w*done_masks for v,w in zip(decoder_hidden, encoder_hidden))\n",
    "    else:\n",
    "        hidden_states = masks * hidden_states\n",
    "    return hidden_states\n",
    "\n",
    "# print(hidden_states[0])\n",
    "# print(encoder_hidden[0])\n",
    "not_done_masks = torch.ones(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "not_done_masks[0] = 0\n",
    "print(not_done_masks)\n",
    "\n",
    "if not isinstance(hidden_states, tuple):\n",
    "    dec_hidden = self._unpack_hidden(hidden_states)\n",
    "\n",
    "# done_masks = 1- not_done_masks\n",
    "# print(not_done_masks.unsqueeze(0))\n",
    "\n",
    "# if not isinstance(hidden_states, tuple):\n",
    "#     hidden_states = self._unpack_hidden(hidden_states)\n",
    "\n",
    "# enc_hidden = mask_hidden(encoder_hidden, done_masks.unsqueeze(0))\n",
    "# dec_hidden = mask_hidden(hidden_states, not_done_masks.unsqueeze(0))\n",
    "\n",
    "# print(enc_hidden.shape)\n",
    "# print(\"-----------------\")\n",
    "# print(dec_hidden.shape)\n",
    "\n",
    "# print(\"-------------------------\")\n",
    "\n",
    "\n",
    "# hidden_states_final = enc_hidden + dec_hidden\n",
    "\n",
    "# print(hidden_states_final)\n",
    "\n",
    "\n",
    "hidden_states = mask_hidden(dec_hidden, encoder_hidden, not_done_masks.unsqueeze(0))\n",
    "\n",
    "print(dec_hidden)\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "print(encoder_hidden)\n",
    "\n",
    "print(\"----------------------\")\n",
    "\n",
    "print(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlnce_baselines.models.decoder.attn_decoder import SoftAttn\n",
    "\n",
    "attn_model = 'general'\n",
    "hidden_size = model_config.STATE_ENCODER.hidden_size\n",
    "decoder_max_len = x.shape[1]\n",
    "# Choose attention model\n",
    "if attn_model != 'none':\n",
    "    attn = SoftAttn(attn_model, hidden_size). to(device)\n",
    "    \n",
    "attn_weights = attn(x, encoder_output,decoder_max_len,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "print(attn_weights.shape)\n",
    "\n",
    "print(attn_weights[3,2,:].sum())\n",
    "\n",
    "concat = nn.Linear(model_config.STATE_ENCODER.hidden_size * 2, model_config.STATE_ENCODER.hidden_size).to(device)\n",
    "\n",
    "context = attn_weights.bmm(encoder_output) # B x S=1 x N\n",
    "print(context.shape)\n",
    "concat_input = torch.cat((x, context), 2)\n",
    "concat_output = F.tanh(concat(concat_input))\n",
    "print(concat_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(rnn_hidden_states.shape)\n",
    "print(encoder_output.shape)\n",
    "print(encoder_hidden[0][-1].shape)\n",
    "print(encoder_hidden[1].shape)\n",
    "\n",
    "print (depth_embedding.shape)\n",
    "print (rgb_embedding.shape)\n",
    "print(x.shape)\n",
    "print(rnn_hidden_states.shape)\n",
    "\n",
    "print(not_done_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vlnce_baselines.common.aux_losses import AuxLosses\n",
    "recurrent_hidden_states = torch.zeros(\n",
    "    actor_critic.net.num_recurrent_layers,\n",
    "    config.NUM_PROCESSES,\n",
    "    config.MODEL.STATE_ENCODER.hidden_size,\n",
    "    device=device,\n",
    ")\n",
    "prev_actions = torch.zeros(\n",
    "    config.NUM_PROCESSES, 1, device=device, dtype=torch.long\n",
    ")\n",
    "not_done_masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "            actor_critic.parameters(), lr=config.DAGGER.LR\n",
    "        )\n",
    "\n",
    "for batch in diter:\n",
    "    (\n",
    "                            observations_batch,\n",
    "                            prev_actions_batch,\n",
    "                            not_done_masks,\n",
    "                            corrected_actions_batch,\n",
    "                            weights_batch,\n",
    "                        ) = batch\n",
    "    observations_batch = {\n",
    "                            k: v.to(device=device, non_blocking=True)\n",
    "                            for k, v in observations_batch.items()\n",
    "                        }\n",
    "    observations, prev_actions, not_done_masks, corrected_actions, weights= observations_batch, prev_actions_batch.to(device=device, non_blocking=True), not_done_masks.to(device=device, non_blocking=True), corrected_actions_batch.to(device=device, non_blocking=True), weights_batch.to(device=device, non_blocking=True)\n",
    "    T, N = corrected_actions.size()\n",
    "    \n",
    "    print(T,N)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    recurrent_hidden_states = torch.zeros(\n",
    "        actor_critic.net.num_recurrent_layers,\n",
    "        N,\n",
    "        config.MODEL.STATE_ENCODER.hidden_size,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "    distribution = actor_critic.build_distribution(\n",
    "        observations, recurrent_hidden_states, prev_actions, not_done_masks\n",
    "    )\n",
    "\n",
    "    logits = distribution.logits\n",
    "#     logits = logits.view(T, N, -1)\n",
    "    print(logits)\n",
    "#     print(logits.permute(0, 2, 1).shape)\n",
    "    corrected_actions = corrected_actions.contiguous().view(T*N)\n",
    "    print(corrected_actions)\n",
    "    action_loss = F.cross_entropy(\n",
    "            logits, corrected_actions, reduction=\"none\"\n",
    "        )\n",
    "\n",
    "#     action_loss = F.cross_entropy(\n",
    "#             logits.permute(0, 2, 1), corrected_actions, reduction=\"none\"\n",
    "#         )\n",
    "\n",
    "    action_loss = action_loss.view(T, N)\n",
    "    \n",
    "    print(action_loss.shape)\n",
    "    print(weights_batch.shape)\n",
    "    print(\"------------------\")\n",
    "    action_loss = ((weights * action_loss).sum(0) / weights.sum(0)).mean()\n",
    "    \n",
    "    \n",
    "\n",
    "    loss = action_loss \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del distribution, logits, loss, action_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recurrent_hidden_states.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vlnce_baselines.common.utils import transform_obs\n",
    "from habitat_baselines.common.utils import batch_obs\n",
    "\n",
    "observations = envs.reset()\n",
    "observations = transform_obs(\n",
    "    observations, config.TASK_CONFIG.TASK.INSTRUCTION_SENSOR_UUID\n",
    ")\n",
    "batch = batch_obs(observations, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(batch['instruction'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def hook_builder(tgt_tensor):\n",
    "#     def hook(m, i, o):\n",
    "#         tgt_tensor.set_(o.cpu())\n",
    "#     return hook\n",
    "# rgb_features = None\n",
    "# rgb_hook = None\n",
    "# if config.MODEL.RGB_ENCODER.cnn_type == \"TorchVisionResNet50\":\n",
    "#     rgb_features = torch.zeros((1,), device=\"cpu\")\n",
    "#     rgb_hook = actor_critic.net.rgb_encoder.layer_extract.register_forward_hook(\n",
    "#         hook_builder(rgb_features)\n",
    "#     )\n",
    "    \n",
    "# sem_map_attn_features = None\n",
    "# sem_map_attn_hook = None\n",
    "# sem_map_attn_features = torch.zeros((1,), device=\"cpu\")\n",
    "# sem_map_attn_hook = actor_critic.net.sem_map_attn_encoder.ego_sem_model.register_forward_hook(\n",
    "#         hook_builder(sem_map_attn_features)\n",
    "#     )\n",
    "\n",
    "recurrent_hidden_states = torch.zeros(\n",
    "    actor_critic.net.num_recurrent_layers,\n",
    "    config.NUM_PROCESSES,\n",
    "    config.MODEL.STATE_ENCODER.hidden_size,\n",
    "    device=device,\n",
    ")\n",
    "prev_actions = torch.zeros(\n",
    "    config.NUM_PROCESSES, 1, device=device, dtype=torch.long\n",
    ")\n",
    "not_done_masks = torch.zeros(config.NUM_PROCESSES, 1, device=device)\n",
    "'''\n",
    "Check if rgb_features is getting updated\n",
    "'''\n",
    "(_, actions, _, recurrent_hidden_states) = actor_critic.act(\n",
    "    batch,\n",
    "    recurrent_hidden_states,\n",
    "    prev_actions,\n",
    "    not_done_masks,\n",
    "    deterministic=False,\n",
    ")\n",
    "\n",
    "print(actions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions.view(-1).shape)\n",
    "print(prev_actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['lang_attention'].shape)\n",
    "outputs = envs.step([a[0].item() for a in actions])\n",
    "observations, _, dones, infos = [list(x) for x in zip(*outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations[2]['rgb'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space= envs.observation_spaces[0]\n",
    "if 'ego_sem_map' in observation_space.spaces:\n",
    "    print('true')\n",
    "    \n",
    "sem_is_color = len(\n",
    "                observation_space.spaces['ego_sem_map'].shape) > 2\n",
    "\n",
    "print(sem_is_color)\n",
    "\n",
    "print(batch[\"ego_sem_map\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prev_actions_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Model Outputs(Instruction Encoder, State encoder, here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from vlnce_baselines.common.aux_losses import AuxLosses\n",
    "import torch.nn as nn\n",
    "\n",
    "observations, prev_actions, not_done_masks, corrected_actions, weights= observations_batch, prev_actions_batch.to(device=device, non_blocking=True), not_done_masks.to(device=device, non_blocking=True), corrected_actions_batch.to(device=device, non_blocking=True), weights_batch.to(device=device, non_blocking=True)\n",
    "T, N = corrected_actions.size()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "prev_actions_list =None\n",
    "AuxLosses.clear()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, reduction=\"none\")\n",
    "\n",
    "distribution = actor_critic.build_distribution(\n",
    "    observations, prev_actions_list, prev_actions, not_done_masks\n",
    ")\n",
    "\n",
    "\n",
    "logits = distribution.logits\n",
    "\n",
    "\n",
    "\n",
    "corrected_actions = corrected_actions.contiguous().view(T*N)\n",
    "action_loss = criterion(logits, corrected_actions)\n",
    "\n",
    "print(\"action loss\",action_loss.shape)\n",
    "print(\"without view mean\",action_loss.mean())\n",
    "\n",
    "\n",
    "action_loss = action_loss.view(T, N)\n",
    "print(((weights * action_loss).sum(0) / weights.sum(0)).shape)\n",
    "action_loss = ((weights * action_loss).sum(0) / weights.sum(0)).mean()\n",
    "\n",
    "print(\"action_loss\",action_loss)\n",
    "\n",
    "# aux_mask = (weights > 0).view(-1)\n",
    "# aux_loss = AuxLosses.reduce(aux_mask)\n",
    "\n",
    "loss = action_loss \n",
    "# loss.backward()\n",
    "\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits)\n",
    "print(corrected_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config ='vlnce_baselines/config/paper_configs/cma.yaml'\n",
    "config = get_config(exp_config, None)\n",
    "\n",
    "from habitat_baselines.common.baseline_registry import baseline_registry\n",
    "trainer_init = baseline_registry.get_trainer(config.TRAINER_NAME)\n",
    "assert trainer_init is not None, f\"{config.TRAINER_NAME} is not supported\"\n",
    "trainer = trainer_init(config)\n",
    "\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "habitat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
